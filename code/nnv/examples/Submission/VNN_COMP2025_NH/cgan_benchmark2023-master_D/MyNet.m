function [Y, state] = MyNet(X, params, varargin)
%MYNET Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 9
%
% Variable names in this function are taken from the original ONNX file.
%
% [Y] = MyNet(X, PARAMS)
%			- Evaluates the imported ONNX network MYNET with input(s)
%			X and the imported network parameters in PARAMS. Returns
%			network output(s) in Y.
%
% [Y, STATE] = MyNet(X, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = MyNet(X, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% X
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  X:		[1, 5]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% Y
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  Y:		[1, 1]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[X, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(X, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'X'}, {X}, [2]);
% Call the top-level graph function:
[Y, YNumDims, state] = torch_jit_exportGraph1000(X, NumDims.X, Vars, NumDims, Training, params.State);
% Postprocess the output data
[Y] = postprocessOutput(Y, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [Y, YNumDims1171, state] = torch_jit_exportGraph1000(X, XNumDims1170, Vars, NumDims, Training, state)
% Function implementing the graph 'torch_jit_exportGraph1000'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.X = X;
NumDims.X = XNumDims1170;

% Execute the operators:
% MatMul:
[Vars.onnx__MatMul_244, NumDims.onnx__MatMul_244] = onnxMatMul(Vars.onnx__MatMul_798, Vars.gen_snlinear0_weight_v, NumDims.onnx__MatMul_798, NumDims.gen_snlinear0_weight_v);

% MatMul:
[Vars.onnx__Div_245, NumDims.onnx__Div_245] = onnxMatMul(Vars.gen_snlinear0_weight_u, Vars.onnx__MatMul_244, NumDims.gen_snlinear0_weight_u, NumDims.onnx__MatMul_244);

% Div:
Vars.onnx__Gemm_246 = Vars.gen_snlinear0_weight_orig ./ Vars.onnx__Div_245;
NumDims.onnx__Gemm_246 = max(NumDims.gen_snlinear0_weight_orig, NumDims.onnx__Div_245);

% Gemm:
[A, B, C, alpha, beta, NumDims.onnx__Reshape_247] = prepareGemmArgs(Vars.X, Vars.onnx__Gemm_246, Vars.gen_snlinear0_bias, Vars.Gemmalpha1001, Vars.Gemmbeta1002, 0, 1, NumDims.gen_snlinear0_bias);
Vars.onnx__Reshape_247 = alpha*B*A + beta*C;

% Reshape:
[shape, NumDims.input] = prepareReshapeArgs(Vars.onnx__Reshape_247, Vars.onnx__Reshape_248, NumDims.onnx__Reshape_247, 0);
Vars.input = reshape(Vars.onnx__Reshape_247, shape{:});

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_3, NumDims.gen_block1_cond_bn1_running_mean, NumDims.gen_block1_cond_bn1_running_var] = prepareBatchNormalizationArgs(Vars.gen_block1_cond_bn1_bias, Vars.gen_block1_cond_bn1_weight, Vars.gen_block1_cond_bn1_running_mean, Vars.gen_block1_cond_bn1_running_var, NumDims.input, NumDims.gen_block1_cond_bn1_running_mean, NumDims.gen_block1_cond_bn1_running_var);
if Training
    [Vars.input_3, dsmean, dsvar] = batchnorm(Vars.input, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_block1_cond_bn1_running_mean = dlarray(dsmean);
    Vars.gen_block1_cond_bn1_running_var = dlarray(dsvar);
else
    Vars.input_3 = batchnorm(Vars.input, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_block1_cond_bn1_running_mean = Vars.gen_block1_cond_bn1_running_mean;
state.gen_block1_cond_bn1_running_var = Vars.gen_block1_cond_bn1_running_var;

% Relu:
Vars.onnx__Upsample_251 = relu(Vars.input_3);
NumDims.onnx__Upsample_251 = NumDims.input_3;

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_7] = prepareResize11Args([], Vars.onnx__Upsample_799, dlarray([]), "half_pixel", "nearest", "round", NumDims.onnx__Upsample_251);
Vars.input_7 = dlresize(Vars.onnx__Upsample_251, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_262, NumDims.onnx__MatMul_262] = onnxMatMul(Vars.onnx__MatMul_803, Vars.gen_block1_snconv2d1_weight_v, NumDims.onnx__MatMul_803, NumDims.gen_block1_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_263, NumDims.onnx__Div_263] = onnxMatMul(Vars.gen_block1_snconv2d1_weight_u, Vars.onnx__MatMul_262, NumDims.gen_block1_snconv2d1_weight_u, NumDims.onnx__MatMul_262);

% Div:
Vars.onnx__Conv_264 = Vars.gen_block1_snconv2d1_weight_orig ./ Vars.onnx__Div_263;
NumDims.onnx__Conv_264 = max(NumDims.gen_block1_snconv2d1_weight_orig, NumDims.onnx__Div_263);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_265] = prepareConvArgs(Vars.onnx__Conv_264, Vars.gen_block1_snconv2d1_bias, Vars.ConvStride1003, Vars.ConvDilationFactor1004, Vars.ConvPadding1005, 1, NumDims.input_7, NumDims.onnx__Conv_264);
Vars.onnx__Add_265 = dlconv(Vars.input_7, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_11] = prepareResize11Args([], Vars.onnx__Upsample_804, dlarray([]), "half_pixel", "nearest", "round", NumDims.input);
Vars.input_11 = dlresize(Vars.input, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_276, NumDims.onnx__MatMul_276] = onnxMatMul(Vars.onnx__MatMul_808, Vars.gen_block1_snconv2d0_weight_v, NumDims.onnx__MatMul_808, NumDims.gen_block1_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_277, NumDims.onnx__Div_277] = onnxMatMul(Vars.gen_block1_snconv2d0_weight_u, Vars.onnx__MatMul_276, NumDims.gen_block1_snconv2d0_weight_u, NumDims.onnx__MatMul_276);

% Div:
Vars.onnx__Conv_278 = Vars.gen_block1_snconv2d0_weight_orig ./ Vars.onnx__Div_277;
NumDims.onnx__Conv_278 = max(NumDims.gen_block1_snconv2d0_weight_orig, NumDims.onnx__Div_277);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_279] = prepareConvArgs(Vars.onnx__Conv_278, Vars.gen_block1_snconv2d0_bias, Vars.ConvStride1006, Vars.ConvDilationFactor1007, Vars.ConvPadding1008, 1, NumDims.input_11, NumDims.onnx__Conv_278);
Vars.onnx__Add_279 = dlconv(Vars.input_11, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_15 = Vars.onnx__Add_265 + Vars.onnx__Add_279;
NumDims.input_15 = max(NumDims.onnx__Add_265, NumDims.onnx__Add_279);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_19, NumDims.gen_block2_cond_bn1_running_mean, NumDims.gen_block2_cond_bn1_running_var] = prepareBatchNormalizationArgs(Vars.gen_block2_cond_bn1_bias, Vars.gen_block2_cond_bn1_weight, Vars.gen_block2_cond_bn1_running_mean, Vars.gen_block2_cond_bn1_running_var, NumDims.input_15, NumDims.gen_block2_cond_bn1_running_mean, NumDims.gen_block2_cond_bn1_running_var);
if Training
    [Vars.input_19, dsmean, dsvar] = batchnorm(Vars.input_15, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_block2_cond_bn1_running_mean = dlarray(dsmean);
    Vars.gen_block2_cond_bn1_running_var = dlarray(dsvar);
else
    Vars.input_19 = batchnorm(Vars.input_15, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_block2_cond_bn1_running_mean = Vars.gen_block2_cond_bn1_running_mean;
state.gen_block2_cond_bn1_running_var = Vars.gen_block2_cond_bn1_running_var;

% Relu:
Vars.onnx__Upsample_282 = relu(Vars.input_19);
NumDims.onnx__Upsample_282 = NumDims.input_19;

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_23] = prepareResize11Args([], Vars.onnx__Upsample_809, dlarray([]), "half_pixel", "nearest", "round", NumDims.onnx__Upsample_282);
Vars.input_23 = dlresize(Vars.onnx__Upsample_282, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_293, NumDims.onnx__MatMul_293] = onnxMatMul(Vars.onnx__MatMul_813, Vars.gen_block2_snconv2d1_weight_v, NumDims.onnx__MatMul_813, NumDims.gen_block2_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_294, NumDims.onnx__Div_294] = onnxMatMul(Vars.gen_block2_snconv2d1_weight_u, Vars.onnx__MatMul_293, NumDims.gen_block2_snconv2d1_weight_u, NumDims.onnx__MatMul_293);

% Div:
Vars.onnx__Conv_295 = Vars.gen_block2_snconv2d1_weight_orig ./ Vars.onnx__Div_294;
NumDims.onnx__Conv_295 = max(NumDims.gen_block2_snconv2d1_weight_orig, NumDims.onnx__Div_294);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_296] = prepareConvArgs(Vars.onnx__Conv_295, Vars.gen_block2_snconv2d1_bias, Vars.ConvStride1009, Vars.ConvDilationFactor1010, Vars.ConvPadding1011, 1, NumDims.input_23, NumDims.onnx__Conv_295);
Vars.onnx__Add_296 = dlconv(Vars.input_23, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_27] = prepareResize11Args([], Vars.onnx__Upsample_814, dlarray([]), "half_pixel", "nearest", "round", NumDims.input_15);
Vars.input_27 = dlresize(Vars.input_15, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_307, NumDims.onnx__MatMul_307] = onnxMatMul(Vars.onnx__MatMul_818, Vars.gen_block2_snconv2d0_weight_v, NumDims.onnx__MatMul_818, NumDims.gen_block2_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_308, NumDims.onnx__Div_308] = onnxMatMul(Vars.gen_block2_snconv2d0_weight_u, Vars.onnx__MatMul_307, NumDims.gen_block2_snconv2d0_weight_u, NumDims.onnx__MatMul_307);

% Div:
Vars.onnx__Conv_309 = Vars.gen_block2_snconv2d0_weight_orig ./ Vars.onnx__Div_308;
NumDims.onnx__Conv_309 = max(NumDims.gen_block2_snconv2d0_weight_orig, NumDims.onnx__Div_308);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_310] = prepareConvArgs(Vars.onnx__Conv_309, Vars.gen_block2_snconv2d0_bias, Vars.ConvStride1012, Vars.ConvDilationFactor1013, Vars.ConvPadding1014, 1, NumDims.input_27, NumDims.onnx__Conv_309);
Vars.onnx__Add_310 = dlconv(Vars.input_27, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_31 = Vars.onnx__Add_296 + Vars.onnx__Add_310;
NumDims.input_31 = max(NumDims.onnx__Add_296, NumDims.onnx__Add_310);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_35, NumDims.gen_block3_cond_bn1_running_mean, NumDims.gen_block3_cond_bn1_running_var] = prepareBatchNormalizationArgs(Vars.gen_block3_cond_bn1_bias, Vars.gen_block3_cond_bn1_weight, Vars.gen_block3_cond_bn1_running_mean, Vars.gen_block3_cond_bn1_running_var, NumDims.input_31, NumDims.gen_block3_cond_bn1_running_mean, NumDims.gen_block3_cond_bn1_running_var);
if Training
    [Vars.input_35, dsmean, dsvar] = batchnorm(Vars.input_31, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_block3_cond_bn1_running_mean = dlarray(dsmean);
    Vars.gen_block3_cond_bn1_running_var = dlarray(dsvar);
else
    Vars.input_35 = batchnorm(Vars.input_31, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_block3_cond_bn1_running_mean = Vars.gen_block3_cond_bn1_running_mean;
state.gen_block3_cond_bn1_running_var = Vars.gen_block3_cond_bn1_running_var;

% Relu:
Vars.onnx__Upsample_313 = relu(Vars.input_35);
NumDims.onnx__Upsample_313 = NumDims.input_35;

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_39] = prepareResize11Args([], Vars.onnx__Upsample_819, dlarray([]), "half_pixel", "nearest", "round", NumDims.onnx__Upsample_313);
Vars.input_39 = dlresize(Vars.onnx__Upsample_313, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_324, NumDims.onnx__MatMul_324] = onnxMatMul(Vars.onnx__MatMul_823, Vars.gen_block3_snconv2d1_weight_v, NumDims.onnx__MatMul_823, NumDims.gen_block3_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_325, NumDims.onnx__Div_325] = onnxMatMul(Vars.gen_block3_snconv2d1_weight_u, Vars.onnx__MatMul_324, NumDims.gen_block3_snconv2d1_weight_u, NumDims.onnx__MatMul_324);

% Div:
Vars.onnx__Conv_326 = Vars.gen_block3_snconv2d1_weight_orig ./ Vars.onnx__Div_325;
NumDims.onnx__Conv_326 = max(NumDims.gen_block3_snconv2d1_weight_orig, NumDims.onnx__Div_325);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_327] = prepareConvArgs(Vars.onnx__Conv_326, Vars.gen_block3_snconv2d1_bias, Vars.ConvStride1015, Vars.ConvDilationFactor1016, Vars.ConvPadding1017, 1, NumDims.input_39, NumDims.onnx__Conv_326);
Vars.onnx__Add_327 = dlconv(Vars.input_39, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_43] = prepareResize11Args([], Vars.onnx__Upsample_824, dlarray([]), "half_pixel", "nearest", "round", NumDims.input_31);
Vars.input_43 = dlresize(Vars.input_31, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_338, NumDims.onnx__MatMul_338] = onnxMatMul(Vars.onnx__MatMul_828, Vars.gen_block3_snconv2d0_weight_v, NumDims.onnx__MatMul_828, NumDims.gen_block3_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_339, NumDims.onnx__Div_339] = onnxMatMul(Vars.gen_block3_snconv2d0_weight_u, Vars.onnx__MatMul_338, NumDims.gen_block3_snconv2d0_weight_u, NumDims.onnx__MatMul_338);

% Div:
Vars.onnx__Conv_340 = Vars.gen_block3_snconv2d0_weight_orig ./ Vars.onnx__Div_339;
NumDims.onnx__Conv_340 = max(NumDims.gen_block3_snconv2d0_weight_orig, NumDims.onnx__Div_339);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_341] = prepareConvArgs(Vars.onnx__Conv_340, Vars.gen_block3_snconv2d0_bias, Vars.ConvStride1018, Vars.ConvDilationFactor1019, Vars.ConvPadding1020, 1, NumDims.input_43, NumDims.onnx__Conv_340);
Vars.onnx__Add_341 = dlconv(Vars.input_43, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_47 = Vars.onnx__Add_327 + Vars.onnx__Add_341;
NumDims.input_47 = max(NumDims.onnx__Add_327, NumDims.onnx__Add_341);

% Shape:
[Vars.onnx__Gather_343, NumDims.onnx__Gather_343] = onnxShape(Vars.input_47, NumDims.input_47, 0, NumDims.input_47+1);

% Gather:
[Vars.onnx__Div_345, NumDims.onnx__Div_345] = onnxGather(Vars.onnx__Gather_343, Vars.onnx__Gather_344, 0, NumDims.onnx__Gather_343, NumDims.onnx__Gather_344);

% Shape:
[Vars.onnx__Gather_346, NumDims.onnx__Gather_346] = onnxShape(Vars.input_47, NumDims.input_47, 0, NumDims.input_47+1);

% Gather:
[Vars.onnx__Mul_348, NumDims.onnx__Mul_348] = onnxGather(Vars.onnx__Gather_346, Vars.onnx__Gather_347, 0, NumDims.onnx__Gather_346, NumDims.onnx__Gather_347);

% Shape:
[Vars.onnx__Gather_349, NumDims.onnx__Gather_349] = onnxShape(Vars.input_47, NumDims.input_47, 0, NumDims.input_47+1);

% Gather:
[Vars.onnx__Mul_351, NumDims.onnx__Mul_351] = onnxGather(Vars.onnx__Gather_349, Vars.onnx__Gather_350, 0, NumDims.onnx__Gather_349, NumDims.onnx__Gather_350);

% MatMul:
[Vars.onnx__MatMul_358, NumDims.onnx__MatMul_358] = onnxMatMul(Vars.onnx__MatMul_832, Vars.gen_self_attn_snconv1x1_theta_weight_v, NumDims.onnx__MatMul_832, NumDims.gen_self_attn_snconv1x1_theta_weight_v);

% MatMul:
[Vars.onnx__Div_359, NumDims.onnx__Div_359] = onnxMatMul(Vars.gen_self_attn_snconv1x1_theta_weight_u, Vars.onnx__MatMul_358, NumDims.gen_self_attn_snconv1x1_theta_weight_u, NumDims.onnx__MatMul_358);

% Div:
Vars.onnx__Conv_360 = Vars.gen_self_attn_snconv1x1_theta_weight_ori ./ Vars.onnx__Div_359;
NumDims.onnx__Conv_360 = max(NumDims.gen_self_attn_snconv1x1_theta_weight_ori, NumDims.onnx__Div_359);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Reshape_361] = prepareConvArgs(Vars.onnx__Conv_360, Vars.gen_self_attn_snconv1x1_theta_bias, Vars.ConvStride1021, Vars.ConvDilationFactor1022, Vars.ConvPadding1023, 1, NumDims.input_47, NumDims.onnx__Conv_360);
Vars.onnx__Reshape_361 = dlconv(Vars.input_47, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_363 = fix(Vars.onnx__Div_345 ./ Vars.onnx__Div_362);
NumDims.onnx__Cast_363 = max(NumDims.onnx__Div_345, NumDims.onnx__Div_362);

% Cast:
if islogical(Vars.onnx__Cast_363)
    Vars.onnx__Cast_363 = single(Vars.onnx__Cast_363);
end
Vars.onnx__Cast_364 = cast(int64(extractdata(Vars.onnx__Cast_363)), 'like', Vars.onnx__Cast_363);
NumDims.onnx__Cast_364 = NumDims.onnx__Cast_363;

% Cast:
if islogical(Vars.onnx__Cast_364)
    Vars.onnx__Cast_364 = single(Vars.onnx__Cast_364);
end
Vars.onnx__Unsqueeze_365 = cast(int64(extractdata(Vars.onnx__Cast_364)), 'like', Vars.onnx__Cast_364);
NumDims.onnx__Unsqueeze_365 = NumDims.onnx__Cast_364;

% Mul:
Vars.onnx__Unsqueeze_366 = Vars.onnx__Mul_348 .* Vars.onnx__Mul_351;
NumDims.onnx__Unsqueeze_366 = max(NumDims.onnx__Mul_348, NumDims.onnx__Mul_351);

% Unsqueeze:
[shape, NumDims.onnx__Concat_369] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_365, Vars.UnsqueezeAxes1024, NumDims.onnx__Unsqueeze_365);
Vars.onnx__Concat_369 = reshape(Vars.onnx__Unsqueeze_365, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_370] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_366, Vars.UnsqueezeAxes1025, NumDims.onnx__Unsqueeze_366);
Vars.onnx__Concat_370 = reshape(Vars.onnx__Unsqueeze_366, shape);

% Concat:
[Vars.onnx__Reshape_371, NumDims.onnx__Reshape_371] = onnxConcat(0, {Vars.onnx__Concat_833, Vars.onnx__Concat_369, Vars.onnx__Concat_370}, [NumDims.onnx__Concat_833, NumDims.onnx__Concat_369, NumDims.onnx__Concat_370]);

% Reshape:
[shape, NumDims.onnx__Transpose_372] = prepareReshapeArgs(Vars.onnx__Reshape_361, Vars.onnx__Reshape_371, NumDims.onnx__Reshape_361, 0);
Vars.onnx__Transpose_372 = reshape(Vars.onnx__Reshape_361, shape{:});

% MatMul:
[Vars.onnx__MatMul_379, NumDims.onnx__MatMul_379] = onnxMatMul(Vars.onnx__MatMul_837, Vars.gen_self_attn_snconv1x1_phi_weight_v, NumDims.onnx__MatMul_837, NumDims.gen_self_attn_snconv1x1_phi_weight_v);

% MatMul:
[Vars.onnx__Div_380, NumDims.onnx__Div_380] = onnxMatMul(Vars.gen_self_attn_snconv1x1_phi_weight_u, Vars.onnx__MatMul_379, NumDims.gen_self_attn_snconv1x1_phi_weight_u, NumDims.onnx__MatMul_379);

% Div:
Vars.onnx__Conv_381 = Vars.gen_self_attn_snconv1x1_phi_weight_orig ./ Vars.onnx__Div_380;
NumDims.onnx__Conv_381 = max(NumDims.gen_self_attn_snconv1x1_phi_weight_orig, NumDims.onnx__Div_380);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__MaxPool_382] = prepareConvArgs(Vars.onnx__Conv_381, Vars.gen_self_attn_snconv1x1_phi_bias, Vars.ConvStride1026, Vars.ConvDilationFactor1027, Vars.ConvPadding1028, 1, NumDims.input_47, NumDims.onnx__Conv_381);
Vars.onnx__MaxPool_382 = dlconv(Vars.input_47, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.onnx__Reshape_383] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1029, Vars.MaxPoolStride1030, Vars.MaxPoolPadding1031, NumDims.onnx__MaxPool_382);
Vars.onnx__Reshape_383 = maxpool(Vars.onnx__MaxPool_382, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_385 = fix(Vars.onnx__Div_345 ./ Vars.onnx__Div_384);
NumDims.onnx__Cast_385 = max(NumDims.onnx__Div_345, NumDims.onnx__Div_384);

% Cast:
if islogical(Vars.onnx__Cast_385)
    Vars.onnx__Cast_385 = single(Vars.onnx__Cast_385);
end
Vars.onnx__Cast_386 = cast(int64(extractdata(Vars.onnx__Cast_385)), 'like', Vars.onnx__Cast_385);
NumDims.onnx__Cast_386 = NumDims.onnx__Cast_385;

% Cast:
if islogical(Vars.onnx__Cast_386)
    Vars.onnx__Cast_386 = single(Vars.onnx__Cast_386);
end
Vars.onnx__Unsqueeze_387 = cast(int64(extractdata(Vars.onnx__Cast_386)), 'like', Vars.onnx__Cast_386);
NumDims.onnx__Unsqueeze_387 = NumDims.onnx__Cast_386;

% Mul:
Vars.onnx__Div_388 = Vars.onnx__Mul_348 .* Vars.onnx__Mul_351;
NumDims.onnx__Div_388 = max(NumDims.onnx__Mul_348, NumDims.onnx__Mul_351);

% Div:
Vars.onnx__Cast_390 = fix(Vars.onnx__Div_388 ./ Vars.onnx__Div_389);
NumDims.onnx__Cast_390 = max(NumDims.onnx__Div_388, NumDims.onnx__Div_389);

% Cast:
if islogical(Vars.onnx__Cast_390)
    Vars.onnx__Cast_390 = single(Vars.onnx__Cast_390);
end
Vars.onnx__Cast_391 = cast(int64(extractdata(Vars.onnx__Cast_390)), 'like', Vars.onnx__Cast_390);
NumDims.onnx__Cast_391 = NumDims.onnx__Cast_390;

% Cast:
if islogical(Vars.onnx__Cast_391)
    Vars.onnx__Cast_391 = single(Vars.onnx__Cast_391);
end
Vars.onnx__Unsqueeze_392 = cast(int64(extractdata(Vars.onnx__Cast_391)), 'like', Vars.onnx__Cast_391);
NumDims.onnx__Unsqueeze_392 = NumDims.onnx__Cast_391;

% Unsqueeze:
[shape, NumDims.onnx__Concat_395] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_387, Vars.UnsqueezeAxes1032, NumDims.onnx__Unsqueeze_387);
Vars.onnx__Concat_395 = reshape(Vars.onnx__Unsqueeze_387, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_396] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_392, Vars.UnsqueezeAxes1033, NumDims.onnx__Unsqueeze_392);
Vars.onnx__Concat_396 = reshape(Vars.onnx__Unsqueeze_392, shape);

% Concat:
[Vars.onnx__Reshape_397, NumDims.onnx__Reshape_397] = onnxConcat(0, {Vars.onnx__Concat_838, Vars.onnx__Concat_395, Vars.onnx__Concat_396}, [NumDims.onnx__Concat_838, NumDims.onnx__Concat_395, NumDims.onnx__Concat_396]);

% Reshape:
[shape, NumDims.onnx__MatMul_398] = prepareReshapeArgs(Vars.onnx__Reshape_383, Vars.onnx__Reshape_397, NumDims.onnx__Reshape_383, 0);
Vars.onnx__MatMul_398 = reshape(Vars.onnx__Reshape_383, shape{:});

% Transpose:
[perm, NumDims.onnx__MatMul_399] = prepareTransposeArgs(Vars.TransposePerm1034, NumDims.onnx__Transpose_372);
if ~isempty(perm)
    Vars.onnx__MatMul_399 = permute(Vars.onnx__Transpose_372, perm);
end

% MatMul:
[Vars.input_51, NumDims.input_51] = onnxMatMul(Vars.onnx__MatMul_399, Vars.onnx__MatMul_398, NumDims.onnx__MatMul_399, NumDims.onnx__MatMul_398);

% Softmax:
[dim1, dim2, origSize, NumDims.onnx__Transpose_401] = prepareSoftmaxArgs(Vars.input_51, 2, NumDims.input_51);
Vars.onnx__Transpose_401 = reshape(Vars.input_51, dim1, dim2);
Vars.onnx__Transpose_401 = softmax(Vars.onnx__Transpose_401, 'DataFormat', 'CB');
Vars.onnx__Transpose_401 = reshape(Vars.onnx__Transpose_401, origSize);

% MatMul:
[Vars.onnx__MatMul_408, NumDims.onnx__MatMul_408] = onnxMatMul(Vars.onnx__MatMul_842, Vars.gen_self_attn_snconv1x1_g_weight_v, NumDims.onnx__MatMul_842, NumDims.gen_self_attn_snconv1x1_g_weight_v);

% MatMul:
[Vars.onnx__Div_409, NumDims.onnx__Div_409] = onnxMatMul(Vars.gen_self_attn_snconv1x1_g_weight_u, Vars.onnx__MatMul_408, NumDims.gen_self_attn_snconv1x1_g_weight_u, NumDims.onnx__MatMul_408);

% Div:
Vars.onnx__Conv_410 = Vars.gen_self_attn_snconv1x1_g_weight_orig ./ Vars.onnx__Div_409;
NumDims.onnx__Conv_410 = max(NumDims.gen_self_attn_snconv1x1_g_weight_orig, NumDims.onnx__Div_409);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__MaxPool_411] = prepareConvArgs(Vars.onnx__Conv_410, Vars.gen_self_attn_snconv1x1_g_bias, Vars.ConvStride1035, Vars.ConvDilationFactor1036, Vars.ConvPadding1037, 1, NumDims.input_47, NumDims.onnx__Conv_410);
Vars.onnx__MaxPool_411 = dlconv(Vars.input_47, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.onnx__Reshape_412] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1038, Vars.MaxPoolStride1039, Vars.MaxPoolPadding1040, NumDims.onnx__MaxPool_411);
Vars.onnx__Reshape_412 = maxpool(Vars.onnx__MaxPool_411, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_414 = fix(Vars.onnx__Div_345 ./ Vars.onnx__Div_413);
NumDims.onnx__Cast_414 = max(NumDims.onnx__Div_345, NumDims.onnx__Div_413);

% Cast:
if islogical(Vars.onnx__Cast_414)
    Vars.onnx__Cast_414 = single(Vars.onnx__Cast_414);
end
Vars.onnx__Cast_415 = cast(int64(extractdata(Vars.onnx__Cast_414)), 'like', Vars.onnx__Cast_414);
NumDims.onnx__Cast_415 = NumDims.onnx__Cast_414;

% Cast:
if islogical(Vars.onnx__Cast_415)
    Vars.onnx__Cast_415 = single(Vars.onnx__Cast_415);
end
Vars.onnx__Unsqueeze_416 = cast(int64(extractdata(Vars.onnx__Cast_415)), 'like', Vars.onnx__Cast_415);
NumDims.onnx__Unsqueeze_416 = NumDims.onnx__Cast_415;

% Mul:
Vars.onnx__Div_417 = Vars.onnx__Mul_348 .* Vars.onnx__Mul_351;
NumDims.onnx__Div_417 = max(NumDims.onnx__Mul_348, NumDims.onnx__Mul_351);

% Div:
Vars.onnx__Cast_419 = fix(Vars.onnx__Div_417 ./ Vars.onnx__Div_418);
NumDims.onnx__Cast_419 = max(NumDims.onnx__Div_417, NumDims.onnx__Div_418);

% Cast:
if islogical(Vars.onnx__Cast_419)
    Vars.onnx__Cast_419 = single(Vars.onnx__Cast_419);
end
Vars.onnx__Cast_420 = cast(int64(extractdata(Vars.onnx__Cast_419)), 'like', Vars.onnx__Cast_419);
NumDims.onnx__Cast_420 = NumDims.onnx__Cast_419;

% Cast:
if islogical(Vars.onnx__Cast_420)
    Vars.onnx__Cast_420 = single(Vars.onnx__Cast_420);
end
Vars.onnx__Unsqueeze_421 = cast(int64(extractdata(Vars.onnx__Cast_420)), 'like', Vars.onnx__Cast_420);
NumDims.onnx__Unsqueeze_421 = NumDims.onnx__Cast_420;

% Unsqueeze:
[shape, NumDims.onnx__Concat_424] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_416, Vars.UnsqueezeAxes1041, NumDims.onnx__Unsqueeze_416);
Vars.onnx__Concat_424 = reshape(Vars.onnx__Unsqueeze_416, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_425] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_421, Vars.UnsqueezeAxes1042, NumDims.onnx__Unsqueeze_421);
Vars.onnx__Concat_425 = reshape(Vars.onnx__Unsqueeze_421, shape);

% Concat:
[Vars.onnx__Reshape_426, NumDims.onnx__Reshape_426] = onnxConcat(0, {Vars.onnx__Concat_843, Vars.onnx__Concat_424, Vars.onnx__Concat_425}, [NumDims.onnx__Concat_843, NumDims.onnx__Concat_424, NumDims.onnx__Concat_425]);

% Reshape:
[shape, NumDims.onnx__MatMul_427] = prepareReshapeArgs(Vars.onnx__Reshape_412, Vars.onnx__Reshape_426, NumDims.onnx__Reshape_412, 0);
Vars.onnx__MatMul_427 = reshape(Vars.onnx__Reshape_412, shape{:});

% Transpose:
[perm, NumDims.onnx__MatMul_428] = prepareTransposeArgs(Vars.TransposePerm1043, NumDims.onnx__Transpose_401);
if ~isempty(perm)
    Vars.onnx__MatMul_428 = permute(Vars.onnx__Transpose_401, perm);
end

% MatMul:
[Vars.onnx__Reshape_429, NumDims.onnx__Reshape_429] = onnxMatMul(Vars.onnx__MatMul_427, Vars.onnx__MatMul_428, NumDims.onnx__MatMul_427, NumDims.onnx__MatMul_428);

% Div:
Vars.onnx__Cast_431 = fix(Vars.onnx__Div_345 ./ Vars.onnx__Div_430);
NumDims.onnx__Cast_431 = max(NumDims.onnx__Div_345, NumDims.onnx__Div_430);

% Cast:
if islogical(Vars.onnx__Cast_431)
    Vars.onnx__Cast_431 = single(Vars.onnx__Cast_431);
end
Vars.onnx__Cast_432 = cast(int64(extractdata(Vars.onnx__Cast_431)), 'like', Vars.onnx__Cast_431);
NumDims.onnx__Cast_432 = NumDims.onnx__Cast_431;

% Cast:
if islogical(Vars.onnx__Cast_432)
    Vars.onnx__Cast_432 = single(Vars.onnx__Cast_432);
end
Vars.onnx__Unsqueeze_433 = cast(int64(extractdata(Vars.onnx__Cast_432)), 'like', Vars.onnx__Cast_432);
NumDims.onnx__Unsqueeze_433 = NumDims.onnx__Cast_432;

% Unsqueeze:
[shape, NumDims.onnx__Concat_436] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_433, Vars.UnsqueezeAxes1044, NumDims.onnx__Unsqueeze_433);
Vars.onnx__Concat_436 = reshape(Vars.onnx__Unsqueeze_433, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_437] = prepareUnsqueezeArgs(Vars.onnx__Mul_348, Vars.UnsqueezeAxes1045, NumDims.onnx__Mul_348);
Vars.onnx__Concat_437 = reshape(Vars.onnx__Mul_348, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_438] = prepareUnsqueezeArgs(Vars.onnx__Mul_351, Vars.UnsqueezeAxes1046, NumDims.onnx__Mul_351);
Vars.onnx__Concat_438 = reshape(Vars.onnx__Mul_351, shape);

% Concat:
[Vars.onnx__Reshape_439, NumDims.onnx__Reshape_439] = onnxConcat(0, {Vars.onnx__Concat_844, Vars.onnx__Concat_436, Vars.onnx__Concat_437, Vars.onnx__Concat_438}, [NumDims.onnx__Concat_844, NumDims.onnx__Concat_436, NumDims.onnx__Concat_437, NumDims.onnx__Concat_438]);

% Reshape:
[shape, NumDims.input_55] = prepareReshapeArgs(Vars.onnx__Reshape_429, Vars.onnx__Reshape_439, NumDims.onnx__Reshape_429, 0);
Vars.input_55 = reshape(Vars.onnx__Reshape_429, shape{:});

% MatMul:
[Vars.onnx__MatMul_447, NumDims.onnx__MatMul_447] = onnxMatMul(Vars.onnx__MatMul_848, Vars.gen_self_attn_snconv1x1_attn_weight_v, NumDims.onnx__MatMul_848, NumDims.gen_self_attn_snconv1x1_attn_weight_v);

% MatMul:
[Vars.onnx__Div_448, NumDims.onnx__Div_448] = onnxMatMul(Vars.gen_self_attn_snconv1x1_attn_weight_u, Vars.onnx__MatMul_447, NumDims.gen_self_attn_snconv1x1_attn_weight_u, NumDims.onnx__MatMul_447);

% Div:
Vars.onnx__Conv_449 = Vars.gen_self_attn_snconv1x1_attn_weight_orig ./ Vars.onnx__Div_448;
NumDims.onnx__Conv_449 = max(NumDims.gen_self_attn_snconv1x1_attn_weight_orig, NumDims.onnx__Div_448);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Mul_450] = prepareConvArgs(Vars.onnx__Conv_449, Vars.gen_self_attn_snconv1x1_attn_bias, Vars.ConvStride1047, Vars.ConvDilationFactor1048, Vars.ConvPadding1049, 1, NumDims.input_55, NumDims.onnx__Conv_449);
Vars.onnx__Mul_450 = dlconv(Vars.input_55, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Mul:
Vars.onnx__Add_451 = Vars.gen_self_attn_sigma .* Vars.onnx__Mul_450;
NumDims.onnx__Add_451 = max(NumDims.gen_self_attn_sigma, NumDims.onnx__Mul_450);

% Add:
Vars.input_59 = Vars.input_47 + Vars.onnx__Add_451;
NumDims.input_59 = max(NumDims.input_47, NumDims.onnx__Add_451);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_63, NumDims.gen_block4_cond_bn1_running_mean, NumDims.gen_block4_cond_bn1_running_var] = prepareBatchNormalizationArgs(Vars.gen_block4_cond_bn1_bias, Vars.gen_block4_cond_bn1_weight, Vars.gen_block4_cond_bn1_running_mean, Vars.gen_block4_cond_bn1_running_var, NumDims.input_59, NumDims.gen_block4_cond_bn1_running_mean, NumDims.gen_block4_cond_bn1_running_var);
if Training
    [Vars.input_63, dsmean, dsvar] = batchnorm(Vars.input_59, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_block4_cond_bn1_running_mean = dlarray(dsmean);
    Vars.gen_block4_cond_bn1_running_var = dlarray(dsvar);
else
    Vars.input_63 = batchnorm(Vars.input_59, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_block4_cond_bn1_running_mean = Vars.gen_block4_cond_bn1_running_mean;
state.gen_block4_cond_bn1_running_var = Vars.gen_block4_cond_bn1_running_var;

% Relu:
Vars.onnx__Upsample_454 = relu(Vars.input_63);
NumDims.onnx__Upsample_454 = NumDims.input_63;

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_67] = prepareResize11Args([], Vars.onnx__Upsample_849, dlarray([]), "half_pixel", "nearest", "round", NumDims.onnx__Upsample_454);
Vars.input_67 = dlresize(Vars.onnx__Upsample_454, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_465, NumDims.onnx__MatMul_465] = onnxMatMul(Vars.onnx__MatMul_853, Vars.gen_block4_snconv2d1_weight_v, NumDims.onnx__MatMul_853, NumDims.gen_block4_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_466, NumDims.onnx__Div_466] = onnxMatMul(Vars.gen_block4_snconv2d1_weight_u, Vars.onnx__MatMul_465, NumDims.gen_block4_snconv2d1_weight_u, NumDims.onnx__MatMul_465);

% Div:
Vars.onnx__Conv_467 = Vars.gen_block4_snconv2d1_weight_orig ./ Vars.onnx__Div_466;
NumDims.onnx__Conv_467 = max(NumDims.gen_block4_snconv2d1_weight_orig, NumDims.onnx__Div_466);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_468] = prepareConvArgs(Vars.onnx__Conv_467, Vars.gen_block4_snconv2d1_bias, Vars.ConvStride1050, Vars.ConvDilationFactor1051, Vars.ConvPadding1052, 1, NumDims.input_67, NumDims.onnx__Conv_467);
Vars.onnx__Add_468 = dlconv(Vars.input_67, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_71] = prepareResize11Args([], Vars.onnx__Upsample_854, dlarray([]), "half_pixel", "nearest", "round", NumDims.input_59);
Vars.input_71 = dlresize(Vars.input_59, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_479, NumDims.onnx__MatMul_479] = onnxMatMul(Vars.onnx__MatMul_858, Vars.gen_block4_snconv2d0_weight_v, NumDims.onnx__MatMul_858, NumDims.gen_block4_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_480, NumDims.onnx__Div_480] = onnxMatMul(Vars.gen_block4_snconv2d0_weight_u, Vars.onnx__MatMul_479, NumDims.gen_block4_snconv2d0_weight_u, NumDims.onnx__MatMul_479);

% Div:
Vars.onnx__Conv_481 = Vars.gen_block4_snconv2d0_weight_orig ./ Vars.onnx__Div_480;
NumDims.onnx__Conv_481 = max(NumDims.gen_block4_snconv2d0_weight_orig, NumDims.onnx__Div_480);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_482] = prepareConvArgs(Vars.onnx__Conv_481, Vars.gen_block4_snconv2d0_bias, Vars.ConvStride1053, Vars.ConvDilationFactor1054, Vars.ConvPadding1055, 1, NumDims.input_71, NumDims.onnx__Conv_481);
Vars.onnx__Add_482 = dlconv(Vars.input_71, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_75 = Vars.onnx__Add_468 + Vars.onnx__Add_482;
NumDims.input_75 = max(NumDims.onnx__Add_468, NumDims.onnx__Add_482);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_79, NumDims.gen_block5_cond_bn1_running_mean, NumDims.gen_block5_cond_bn1_running_var] = prepareBatchNormalizationArgs(Vars.gen_block5_cond_bn1_bias, Vars.gen_block5_cond_bn1_weight, Vars.gen_block5_cond_bn1_running_mean, Vars.gen_block5_cond_bn1_running_var, NumDims.input_75, NumDims.gen_block5_cond_bn1_running_mean, NumDims.gen_block5_cond_bn1_running_var);
if Training
    [Vars.input_79, dsmean, dsvar] = batchnorm(Vars.input_75, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_block5_cond_bn1_running_mean = dlarray(dsmean);
    Vars.gen_block5_cond_bn1_running_var = dlarray(dsvar);
else
    Vars.input_79 = batchnorm(Vars.input_75, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_block5_cond_bn1_running_mean = Vars.gen_block5_cond_bn1_running_mean;
state.gen_block5_cond_bn1_running_var = Vars.gen_block5_cond_bn1_running_var;

% Relu:
Vars.onnx__Upsample_485 = relu(Vars.input_79);
NumDims.onnx__Upsample_485 = NumDims.input_79;

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_83] = prepareResize11Args([], Vars.onnx__Upsample_859, dlarray([]), "half_pixel", "nearest", "round", NumDims.onnx__Upsample_485);
Vars.input_83 = dlresize(Vars.onnx__Upsample_485, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_496, NumDims.onnx__MatMul_496] = onnxMatMul(Vars.onnx__MatMul_863, Vars.gen_block5_snconv2d1_weight_v, NumDims.onnx__MatMul_863, NumDims.gen_block5_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_497, NumDims.onnx__Div_497] = onnxMatMul(Vars.gen_block5_snconv2d1_weight_u, Vars.onnx__MatMul_496, NumDims.gen_block5_snconv2d1_weight_u, NumDims.onnx__MatMul_496);

% Div:
Vars.onnx__Conv_498 = Vars.gen_block5_snconv2d1_weight_orig ./ Vars.onnx__Div_497;
NumDims.onnx__Conv_498 = max(NumDims.gen_block5_snconv2d1_weight_orig, NumDims.onnx__Div_497);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_499] = prepareConvArgs(Vars.onnx__Conv_498, Vars.gen_block5_snconv2d1_bias, Vars.ConvStride1056, Vars.ConvDilationFactor1057, Vars.ConvPadding1058, 1, NumDims.input_83, NumDims.onnx__Conv_498);
Vars.onnx__Add_499 = dlconv(Vars.input_83, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Upsample:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.input_87] = prepareResize11Args([], Vars.onnx__Upsample_864, dlarray([]), "half_pixel", "nearest", "round", NumDims.input_75);
Vars.input_87 = dlresize(Vars.input_75, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);

% MatMul:
[Vars.onnx__MatMul_510, NumDims.onnx__MatMul_510] = onnxMatMul(Vars.onnx__MatMul_868, Vars.gen_block5_snconv2d0_weight_v, NumDims.onnx__MatMul_868, NumDims.gen_block5_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_511, NumDims.onnx__Div_511] = onnxMatMul(Vars.gen_block5_snconv2d0_weight_u, Vars.onnx__MatMul_510, NumDims.gen_block5_snconv2d0_weight_u, NumDims.onnx__MatMul_510);

% Div:
Vars.onnx__Conv_512 = Vars.gen_block5_snconv2d0_weight_orig ./ Vars.onnx__Div_511;
NumDims.onnx__Conv_512 = max(NumDims.gen_block5_snconv2d0_weight_orig, NumDims.onnx__Div_511);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_513] = prepareConvArgs(Vars.onnx__Conv_512, Vars.gen_block5_snconv2d0_bias, Vars.ConvStride1059, Vars.ConvDilationFactor1060, Vars.ConvPadding1061, 1, NumDims.input_87, NumDims.onnx__Conv_512);
Vars.onnx__Add_513 = dlconv(Vars.input_87, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_91 = Vars.onnx__Add_499 + Vars.onnx__Add_513;
NumDims.input_91 = max(NumDims.onnx__Add_499, NumDims.onnx__Add_513);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.input_95, NumDims.gen_bn_running_mean, NumDims.gen_bn_running_var] = prepareBatchNormalizationArgs(Vars.gen_bn_bias, Vars.gen_bn_weight, Vars.gen_bn_running_mean, Vars.gen_bn_running_var, NumDims.input_91, NumDims.gen_bn_running_mean, NumDims.gen_bn_running_var);
if Training
    [Vars.input_95, dsmean, dsvar] = batchnorm(Vars.input_91, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
    Vars.gen_bn_running_mean = dlarray(dsmean);
    Vars.gen_bn_running_var = dlarray(dsvar);
else
    Vars.input_95 = batchnorm(Vars.input_91, offset, scale, datasetMean, datasetVariance, 'Epsilon', 1.000000e-05, 'DataFormat', dataFormat);
end
state.gen_bn_running_mean = Vars.gen_bn_running_mean;
state.gen_bn_running_var = Vars.gen_bn_running_var;

% Relu:
Vars.onnx__Conv_516 = relu(Vars.input_95);
NumDims.onnx__Conv_516 = NumDims.input_95;

% MatMul:
[Vars.onnx__MatMul_523, NumDims.onnx__MatMul_523] = onnxMatMul(Vars.onnx__MatMul_872, Vars.gen_snconv2d1_weight_v, NumDims.onnx__MatMul_872, NumDims.gen_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_524, NumDims.onnx__Div_524] = onnxMatMul(Vars.gen_snconv2d1_weight_u, Vars.onnx__MatMul_523, NumDims.gen_snconv2d1_weight_u, NumDims.onnx__MatMul_523);

% Div:
Vars.onnx__Conv_525 = Vars.gen_snconv2d1_weight_orig ./ Vars.onnx__Div_524;
NumDims.onnx__Conv_525 = max(NumDims.gen_snconv2d1_weight_orig, NumDims.onnx__Div_524);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Tanh_526] = prepareConvArgs(Vars.onnx__Conv_525, Vars.gen_snconv2d1_bias, Vars.ConvStride1062, Vars.ConvDilationFactor1063, Vars.ConvPadding1064, 1, NumDims.onnx__Conv_516, NumDims.onnx__Conv_525);
Vars.onnx__Tanh_526 = dlconv(Vars.onnx__Conv_516, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Tanh:
Vars.input_99 = tanh(Vars.onnx__Tanh_526);
NumDims.input_99 = NumDims.onnx__Tanh_526;

% MatMul:
[Vars.onnx__MatMul_534, NumDims.onnx__MatMul_534] = onnxMatMul(Vars.onnx__MatMul_876, Vars.dis_opt_block1_snconv2d1_weight_v, NumDims.onnx__MatMul_876, NumDims.dis_opt_block1_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_535, NumDims.onnx__Div_535] = onnxMatMul(Vars.dis_opt_block1_snconv2d1_weight_u, Vars.onnx__MatMul_534, NumDims.dis_opt_block1_snconv2d1_weight_u, NumDims.onnx__MatMul_534);

% Div:
Vars.onnx__Conv_536 = Vars.dis_opt_block1_snconv2d1_weight_orig ./ Vars.onnx__Div_535;
NumDims.onnx__Conv_536 = max(NumDims.dis_opt_block1_snconv2d1_weight_orig, NumDims.onnx__Div_535);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.input_103] = prepareConvArgs(Vars.onnx__Conv_536, Vars.dis_opt_block1_snconv2d1_bias, Vars.ConvStride1065, Vars.ConvDilationFactor1066, Vars.ConvPadding1067, 1, NumDims.input_99, NumDims.onnx__Conv_536);
Vars.input_103 = dlconv(Vars.input_99, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.onnx__Pad_538 = relu(Vars.input_103);
NumDims.onnx__Pad_538 = NumDims.input_103;

% Pad:
[Vars.onnx__AveragePool_539, NumDims.onnx__AveragePool_539] = onnxPad(Vars.onnx__Pad_538, Vars.PadPadding1068, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_538]'), NumDims.onnx__Pad_538);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_540] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1069, Vars.AveragePoolStride1070, Vars.AveragePoolPadding1071, 0, NumDims.onnx__AveragePool_539);
Vars.onnx__Add_540 = avgpool(Vars.onnx__AveragePool_539, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_541, NumDims.onnx__AveragePool_541] = onnxPad(Vars.input_99, Vars.PadPadding1072, 0.000000, 'constant', dlarray([0:NumDims.input_99]'), NumDims.input_99);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.input_107] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1073, Vars.AveragePoolStride1074, Vars.AveragePoolPadding1075, 0, NumDims.onnx__AveragePool_541);
Vars.input_107 = avgpool(Vars.onnx__AveragePool_541, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% MatMul:
[Vars.onnx__MatMul_549, NumDims.onnx__MatMul_549] = onnxMatMul(Vars.onnx__MatMul_880, Vars.dis_opt_block1_snconv2d0_weight_v, NumDims.onnx__MatMul_880, NumDims.dis_opt_block1_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_550, NumDims.onnx__Div_550] = onnxMatMul(Vars.dis_opt_block1_snconv2d0_weight_u, Vars.onnx__MatMul_549, NumDims.dis_opt_block1_snconv2d0_weight_u, NumDims.onnx__MatMul_549);

% Div:
Vars.onnx__Conv_551 = Vars.dis_opt_block1_snconv2d0_weight_orig ./ Vars.onnx__Div_550;
NumDims.onnx__Conv_551 = max(NumDims.dis_opt_block1_snconv2d0_weight_orig, NumDims.onnx__Div_550);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_552] = prepareConvArgs(Vars.onnx__Conv_551, Vars.dis_opt_block1_snconv2d0_bias, Vars.ConvStride1076, Vars.ConvDilationFactor1077, Vars.ConvPadding1078, 1, NumDims.input_107, NumDims.onnx__Conv_551);
Vars.onnx__Add_552 = dlconv(Vars.input_107, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_111 = Vars.onnx__Add_540 + Vars.onnx__Add_552;
NumDims.input_111 = max(NumDims.onnx__Add_540, NumDims.onnx__Add_552);

% Relu:
Vars.onnx__Conv_554 = relu(Vars.input_111);
NumDims.onnx__Conv_554 = NumDims.input_111;

% MatMul:
[Vars.onnx__MatMul_561, NumDims.onnx__MatMul_561] = onnxMatMul(Vars.onnx__MatMul_884, Vars.dis_block1_snconv2d1_weight_v, NumDims.onnx__MatMul_884, NumDims.dis_block1_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_562, NumDims.onnx__Div_562] = onnxMatMul(Vars.dis_block1_snconv2d1_weight_u, Vars.onnx__MatMul_561, NumDims.dis_block1_snconv2d1_weight_u, NumDims.onnx__MatMul_561);

% Div:
Vars.onnx__Conv_563 = Vars.dis_block1_snconv2d1_weight_orig ./ Vars.onnx__Div_562;
NumDims.onnx__Conv_563 = max(NumDims.dis_block1_snconv2d1_weight_orig, NumDims.onnx__Div_562);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_564] = prepareConvArgs(Vars.onnx__Conv_563, Vars.dis_block1_snconv2d1_bias, Vars.ConvStride1079, Vars.ConvDilationFactor1080, Vars.ConvPadding1081, 1, NumDims.onnx__Conv_554, NumDims.onnx__Conv_563);
Vars.onnx__Pad_564 = dlconv(Vars.onnx__Conv_554, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_565, NumDims.onnx__AveragePool_565] = onnxPad(Vars.onnx__Pad_564, Vars.PadPadding1082, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_564]'), NumDims.onnx__Pad_564);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_566] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1083, Vars.AveragePoolStride1084, Vars.AveragePoolPadding1085, 0, NumDims.onnx__AveragePool_565);
Vars.onnx__Add_566 = avgpool(Vars.onnx__AveragePool_565, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% MatMul:
[Vars.onnx__MatMul_573, NumDims.onnx__MatMul_573] = onnxMatMul(Vars.onnx__MatMul_888, Vars.dis_block1_snconv2d0_weight_v, NumDims.onnx__MatMul_888, NumDims.dis_block1_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_574, NumDims.onnx__Div_574] = onnxMatMul(Vars.dis_block1_snconv2d0_weight_u, Vars.onnx__MatMul_573, NumDims.dis_block1_snconv2d0_weight_u, NumDims.onnx__MatMul_573);

% Div:
Vars.onnx__Conv_575 = Vars.dis_block1_snconv2d0_weight_orig ./ Vars.onnx__Div_574;
NumDims.onnx__Conv_575 = max(NumDims.dis_block1_snconv2d0_weight_orig, NumDims.onnx__Div_574);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_576] = prepareConvArgs(Vars.onnx__Conv_575, Vars.dis_block1_snconv2d0_bias, Vars.ConvStride1086, Vars.ConvDilationFactor1087, Vars.ConvPadding1088, 1, NumDims.onnx__Conv_554, NumDims.onnx__Conv_575);
Vars.onnx__Pad_576 = dlconv(Vars.onnx__Conv_554, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_577, NumDims.onnx__AveragePool_577] = onnxPad(Vars.onnx__Pad_576, Vars.PadPadding1089, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_576]'), NumDims.onnx__Pad_576);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_578] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1090, Vars.AveragePoolStride1091, Vars.AveragePoolPadding1092, 0, NumDims.onnx__AveragePool_577);
Vars.onnx__Add_578 = avgpool(Vars.onnx__AveragePool_577, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Add:
Vars.input_115 = Vars.onnx__Add_566 + Vars.onnx__Add_578;
NumDims.input_115 = max(NumDims.onnx__Add_566, NumDims.onnx__Add_578);

% Shape:
[Vars.onnx__Gather_580, NumDims.onnx__Gather_580] = onnxShape(Vars.input_115, NumDims.input_115, 0, NumDims.input_115+1);

% Gather:
[Vars.onnx__Div_582, NumDims.onnx__Div_582] = onnxGather(Vars.onnx__Gather_580, Vars.onnx__Gather_581, 0, NumDims.onnx__Gather_580, NumDims.onnx__Gather_581);

% Shape:
[Vars.onnx__Gather_583, NumDims.onnx__Gather_583] = onnxShape(Vars.input_115, NumDims.input_115, 0, NumDims.input_115+1);

% Gather:
[Vars.onnx__Mul_585, NumDims.onnx__Mul_585] = onnxGather(Vars.onnx__Gather_583, Vars.onnx__Gather_584, 0, NumDims.onnx__Gather_583, NumDims.onnx__Gather_584);

% Shape:
[Vars.onnx__Gather_586, NumDims.onnx__Gather_586] = onnxShape(Vars.input_115, NumDims.input_115, 0, NumDims.input_115+1);

% Gather:
[Vars.onnx__Mul_588, NumDims.onnx__Mul_588] = onnxGather(Vars.onnx__Gather_586, Vars.onnx__Gather_587, 0, NumDims.onnx__Gather_586, NumDims.onnx__Gather_587);

% MatMul:
[Vars.onnx__MatMul_595, NumDims.onnx__MatMul_595] = onnxMatMul(Vars.onnx__MatMul_892, Vars.dis_self_attn_snconv1x1_theta_weight_v, NumDims.onnx__MatMul_892, NumDims.dis_self_attn_snconv1x1_theta_weight_v);

% MatMul:
[Vars.onnx__Div_596, NumDims.onnx__Div_596] = onnxMatMul(Vars.dis_self_attn_snconv1x1_theta_weight_u, Vars.onnx__MatMul_595, NumDims.dis_self_attn_snconv1x1_theta_weight_u, NumDims.onnx__MatMul_595);

% Div:
Vars.onnx__Conv_597 = Vars.dis_self_attn_snconv1x1_theta_weight_ori ./ Vars.onnx__Div_596;
NumDims.onnx__Conv_597 = max(NumDims.dis_self_attn_snconv1x1_theta_weight_ori, NumDims.onnx__Div_596);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Reshape_598] = prepareConvArgs(Vars.onnx__Conv_597, Vars.dis_self_attn_snconv1x1_theta_bias, Vars.ConvStride1093, Vars.ConvDilationFactor1094, Vars.ConvPadding1095, 1, NumDims.input_115, NumDims.onnx__Conv_597);
Vars.onnx__Reshape_598 = dlconv(Vars.input_115, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_600 = fix(Vars.onnx__Div_582 ./ Vars.onnx__Div_599);
NumDims.onnx__Cast_600 = max(NumDims.onnx__Div_582, NumDims.onnx__Div_599);

% Cast:
if islogical(Vars.onnx__Cast_600)
    Vars.onnx__Cast_600 = single(Vars.onnx__Cast_600);
end
Vars.onnx__Cast_601 = cast(int64(extractdata(Vars.onnx__Cast_600)), 'like', Vars.onnx__Cast_600);
NumDims.onnx__Cast_601 = NumDims.onnx__Cast_600;

% Cast:
if islogical(Vars.onnx__Cast_601)
    Vars.onnx__Cast_601 = single(Vars.onnx__Cast_601);
end
Vars.onnx__Unsqueeze_602 = cast(int64(extractdata(Vars.onnx__Cast_601)), 'like', Vars.onnx__Cast_601);
NumDims.onnx__Unsqueeze_602 = NumDims.onnx__Cast_601;

% Mul:
Vars.onnx__Unsqueeze_603 = Vars.onnx__Mul_585 .* Vars.onnx__Mul_588;
NumDims.onnx__Unsqueeze_603 = max(NumDims.onnx__Mul_585, NumDims.onnx__Mul_588);

% Unsqueeze:
[shape, NumDims.onnx__Concat_606] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_602, Vars.UnsqueezeAxes1096, NumDims.onnx__Unsqueeze_602);
Vars.onnx__Concat_606 = reshape(Vars.onnx__Unsqueeze_602, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_607] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_603, Vars.UnsqueezeAxes1097, NumDims.onnx__Unsqueeze_603);
Vars.onnx__Concat_607 = reshape(Vars.onnx__Unsqueeze_603, shape);

% Concat:
[Vars.onnx__Reshape_608, NumDims.onnx__Reshape_608] = onnxConcat(0, {Vars.onnx__Concat_893, Vars.onnx__Concat_606, Vars.onnx__Concat_607}, [NumDims.onnx__Concat_893, NumDims.onnx__Concat_606, NumDims.onnx__Concat_607]);

% Reshape:
[shape, NumDims.onnx__Transpose_609] = prepareReshapeArgs(Vars.onnx__Reshape_598, Vars.onnx__Reshape_608, NumDims.onnx__Reshape_598, 0);
Vars.onnx__Transpose_609 = reshape(Vars.onnx__Reshape_598, shape{:});

% MatMul:
[Vars.onnx__MatMul_616, NumDims.onnx__MatMul_616] = onnxMatMul(Vars.onnx__MatMul_897, Vars.dis_self_attn_snconv1x1_phi_weight_v, NumDims.onnx__MatMul_897, NumDims.dis_self_attn_snconv1x1_phi_weight_v);

% MatMul:
[Vars.onnx__Div_617, NumDims.onnx__Div_617] = onnxMatMul(Vars.dis_self_attn_snconv1x1_phi_weight_u, Vars.onnx__MatMul_616, NumDims.dis_self_attn_snconv1x1_phi_weight_u, NumDims.onnx__MatMul_616);

% Div:
Vars.onnx__Conv_618 = Vars.dis_self_attn_snconv1x1_phi_weight_orig ./ Vars.onnx__Div_617;
NumDims.onnx__Conv_618 = max(NumDims.dis_self_attn_snconv1x1_phi_weight_orig, NumDims.onnx__Div_617);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__MaxPool_619] = prepareConvArgs(Vars.onnx__Conv_618, Vars.dis_self_attn_snconv1x1_phi_bias, Vars.ConvStride1098, Vars.ConvDilationFactor1099, Vars.ConvPadding1100, 1, NumDims.input_115, NumDims.onnx__Conv_618);
Vars.onnx__MaxPool_619 = dlconv(Vars.input_115, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.onnx__Reshape_620] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1101, Vars.MaxPoolStride1102, Vars.MaxPoolPadding1103, NumDims.onnx__MaxPool_619);
Vars.onnx__Reshape_620 = maxpool(Vars.onnx__MaxPool_619, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_622 = fix(Vars.onnx__Div_582 ./ Vars.onnx__Div_621);
NumDims.onnx__Cast_622 = max(NumDims.onnx__Div_582, NumDims.onnx__Div_621);

% Cast:
if islogical(Vars.onnx__Cast_622)
    Vars.onnx__Cast_622 = single(Vars.onnx__Cast_622);
end
Vars.onnx__Cast_623 = cast(int64(extractdata(Vars.onnx__Cast_622)), 'like', Vars.onnx__Cast_622);
NumDims.onnx__Cast_623 = NumDims.onnx__Cast_622;

% Cast:
if islogical(Vars.onnx__Cast_623)
    Vars.onnx__Cast_623 = single(Vars.onnx__Cast_623);
end
Vars.onnx__Unsqueeze_624 = cast(int64(extractdata(Vars.onnx__Cast_623)), 'like', Vars.onnx__Cast_623);
NumDims.onnx__Unsqueeze_624 = NumDims.onnx__Cast_623;

% Mul:
Vars.onnx__Div_625 = Vars.onnx__Mul_585 .* Vars.onnx__Mul_588;
NumDims.onnx__Div_625 = max(NumDims.onnx__Mul_585, NumDims.onnx__Mul_588);

% Div:
Vars.onnx__Cast_627 = fix(Vars.onnx__Div_625 ./ Vars.onnx__Div_626);
NumDims.onnx__Cast_627 = max(NumDims.onnx__Div_625, NumDims.onnx__Div_626);

% Cast:
if islogical(Vars.onnx__Cast_627)
    Vars.onnx__Cast_627 = single(Vars.onnx__Cast_627);
end
Vars.onnx__Cast_628 = cast(int64(extractdata(Vars.onnx__Cast_627)), 'like', Vars.onnx__Cast_627);
NumDims.onnx__Cast_628 = NumDims.onnx__Cast_627;

% Cast:
if islogical(Vars.onnx__Cast_628)
    Vars.onnx__Cast_628 = single(Vars.onnx__Cast_628);
end
Vars.onnx__Unsqueeze_629 = cast(int64(extractdata(Vars.onnx__Cast_628)), 'like', Vars.onnx__Cast_628);
NumDims.onnx__Unsqueeze_629 = NumDims.onnx__Cast_628;

% Unsqueeze:
[shape, NumDims.onnx__Concat_632] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_624, Vars.UnsqueezeAxes1104, NumDims.onnx__Unsqueeze_624);
Vars.onnx__Concat_632 = reshape(Vars.onnx__Unsqueeze_624, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_633] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_629, Vars.UnsqueezeAxes1105, NumDims.onnx__Unsqueeze_629);
Vars.onnx__Concat_633 = reshape(Vars.onnx__Unsqueeze_629, shape);

% Concat:
[Vars.onnx__Reshape_634, NumDims.onnx__Reshape_634] = onnxConcat(0, {Vars.onnx__Concat_898, Vars.onnx__Concat_632, Vars.onnx__Concat_633}, [NumDims.onnx__Concat_898, NumDims.onnx__Concat_632, NumDims.onnx__Concat_633]);

% Reshape:
[shape, NumDims.onnx__MatMul_635] = prepareReshapeArgs(Vars.onnx__Reshape_620, Vars.onnx__Reshape_634, NumDims.onnx__Reshape_620, 0);
Vars.onnx__MatMul_635 = reshape(Vars.onnx__Reshape_620, shape{:});

% Transpose:
[perm, NumDims.onnx__MatMul_636] = prepareTransposeArgs(Vars.TransposePerm1106, NumDims.onnx__Transpose_609);
if ~isempty(perm)
    Vars.onnx__MatMul_636 = permute(Vars.onnx__Transpose_609, perm);
end

% MatMul:
[Vars.input_119, NumDims.input_119] = onnxMatMul(Vars.onnx__MatMul_636, Vars.onnx__MatMul_635, NumDims.onnx__MatMul_636, NumDims.onnx__MatMul_635);

% Softmax:
[dim1, dim2, origSize, NumDims.onnx__Transpose_638] = prepareSoftmaxArgs(Vars.input_119, 2, NumDims.input_119);
Vars.onnx__Transpose_638 = reshape(Vars.input_119, dim1, dim2);
Vars.onnx__Transpose_638 = softmax(Vars.onnx__Transpose_638, 'DataFormat', 'CB');
Vars.onnx__Transpose_638 = reshape(Vars.onnx__Transpose_638, origSize);

% MatMul:
[Vars.onnx__MatMul_645, NumDims.onnx__MatMul_645] = onnxMatMul(Vars.onnx__MatMul_902, Vars.dis_self_attn_snconv1x1_g_weight_v, NumDims.onnx__MatMul_902, NumDims.dis_self_attn_snconv1x1_g_weight_v);

% MatMul:
[Vars.onnx__Div_646, NumDims.onnx__Div_646] = onnxMatMul(Vars.dis_self_attn_snconv1x1_g_weight_u, Vars.onnx__MatMul_645, NumDims.dis_self_attn_snconv1x1_g_weight_u, NumDims.onnx__MatMul_645);

% Div:
Vars.onnx__Conv_647 = Vars.dis_self_attn_snconv1x1_g_weight_orig ./ Vars.onnx__Div_646;
NumDims.onnx__Conv_647 = max(NumDims.dis_self_attn_snconv1x1_g_weight_orig, NumDims.onnx__Div_646);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__MaxPool_648] = prepareConvArgs(Vars.onnx__Conv_647, Vars.dis_self_attn_snconv1x1_g_bias, Vars.ConvStride1107, Vars.ConvDilationFactor1108, Vars.ConvPadding1109, 1, NumDims.input_115, NumDims.onnx__Conv_647);
Vars.onnx__MaxPool_648 = dlconv(Vars.input_115, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.onnx__Reshape_649] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1110, Vars.MaxPoolStride1111, Vars.MaxPoolPadding1112, NumDims.onnx__MaxPool_648);
Vars.onnx__Reshape_649 = maxpool(Vars.onnx__MaxPool_648, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Div:
Vars.onnx__Cast_651 = fix(Vars.onnx__Div_582 ./ Vars.onnx__Div_650);
NumDims.onnx__Cast_651 = max(NumDims.onnx__Div_582, NumDims.onnx__Div_650);

% Cast:
if islogical(Vars.onnx__Cast_651)
    Vars.onnx__Cast_651 = single(Vars.onnx__Cast_651);
end
Vars.onnx__Cast_652 = cast(int64(extractdata(Vars.onnx__Cast_651)), 'like', Vars.onnx__Cast_651);
NumDims.onnx__Cast_652 = NumDims.onnx__Cast_651;

% Cast:
if islogical(Vars.onnx__Cast_652)
    Vars.onnx__Cast_652 = single(Vars.onnx__Cast_652);
end
Vars.onnx__Unsqueeze_653 = cast(int64(extractdata(Vars.onnx__Cast_652)), 'like', Vars.onnx__Cast_652);
NumDims.onnx__Unsqueeze_653 = NumDims.onnx__Cast_652;

% Mul:
Vars.onnx__Div_654 = Vars.onnx__Mul_585 .* Vars.onnx__Mul_588;
NumDims.onnx__Div_654 = max(NumDims.onnx__Mul_585, NumDims.onnx__Mul_588);

% Div:
Vars.onnx__Cast_656 = fix(Vars.onnx__Div_654 ./ Vars.onnx__Div_655);
NumDims.onnx__Cast_656 = max(NumDims.onnx__Div_654, NumDims.onnx__Div_655);

% Cast:
if islogical(Vars.onnx__Cast_656)
    Vars.onnx__Cast_656 = single(Vars.onnx__Cast_656);
end
Vars.onnx__Cast_657 = cast(int64(extractdata(Vars.onnx__Cast_656)), 'like', Vars.onnx__Cast_656);
NumDims.onnx__Cast_657 = NumDims.onnx__Cast_656;

% Cast:
if islogical(Vars.onnx__Cast_657)
    Vars.onnx__Cast_657 = single(Vars.onnx__Cast_657);
end
Vars.onnx__Unsqueeze_658 = cast(int64(extractdata(Vars.onnx__Cast_657)), 'like', Vars.onnx__Cast_657);
NumDims.onnx__Unsqueeze_658 = NumDims.onnx__Cast_657;

% Unsqueeze:
[shape, NumDims.onnx__Concat_661] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_653, Vars.UnsqueezeAxes1113, NumDims.onnx__Unsqueeze_653);
Vars.onnx__Concat_661 = reshape(Vars.onnx__Unsqueeze_653, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_662] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_658, Vars.UnsqueezeAxes1114, NumDims.onnx__Unsqueeze_658);
Vars.onnx__Concat_662 = reshape(Vars.onnx__Unsqueeze_658, shape);

% Concat:
[Vars.onnx__Reshape_663, NumDims.onnx__Reshape_663] = onnxConcat(0, {Vars.onnx__Concat_903, Vars.onnx__Concat_661, Vars.onnx__Concat_662}, [NumDims.onnx__Concat_903, NumDims.onnx__Concat_661, NumDims.onnx__Concat_662]);

% Reshape:
[shape, NumDims.onnx__MatMul_664] = prepareReshapeArgs(Vars.onnx__Reshape_649, Vars.onnx__Reshape_663, NumDims.onnx__Reshape_649, 0);
Vars.onnx__MatMul_664 = reshape(Vars.onnx__Reshape_649, shape{:});

% Transpose:
[perm, NumDims.onnx__MatMul_665] = prepareTransposeArgs(Vars.TransposePerm1115, NumDims.onnx__Transpose_638);
if ~isempty(perm)
    Vars.onnx__MatMul_665 = permute(Vars.onnx__Transpose_638, perm);
end

% MatMul:
[Vars.onnx__Reshape_666, NumDims.onnx__Reshape_666] = onnxMatMul(Vars.onnx__MatMul_664, Vars.onnx__MatMul_665, NumDims.onnx__MatMul_664, NumDims.onnx__MatMul_665);

% Div:
Vars.onnx__Cast_668 = fix(Vars.onnx__Div_582 ./ Vars.onnx__Div_667);
NumDims.onnx__Cast_668 = max(NumDims.onnx__Div_582, NumDims.onnx__Div_667);

% Cast:
if islogical(Vars.onnx__Cast_668)
    Vars.onnx__Cast_668 = single(Vars.onnx__Cast_668);
end
Vars.onnx__Cast_669 = cast(int64(extractdata(Vars.onnx__Cast_668)), 'like', Vars.onnx__Cast_668);
NumDims.onnx__Cast_669 = NumDims.onnx__Cast_668;

% Cast:
if islogical(Vars.onnx__Cast_669)
    Vars.onnx__Cast_669 = single(Vars.onnx__Cast_669);
end
Vars.onnx__Unsqueeze_670 = cast(int64(extractdata(Vars.onnx__Cast_669)), 'like', Vars.onnx__Cast_669);
NumDims.onnx__Unsqueeze_670 = NumDims.onnx__Cast_669;

% Unsqueeze:
[shape, NumDims.onnx__Concat_673] = prepareUnsqueezeArgs(Vars.onnx__Unsqueeze_670, Vars.UnsqueezeAxes1116, NumDims.onnx__Unsqueeze_670);
Vars.onnx__Concat_673 = reshape(Vars.onnx__Unsqueeze_670, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_674] = prepareUnsqueezeArgs(Vars.onnx__Mul_585, Vars.UnsqueezeAxes1117, NumDims.onnx__Mul_585);
Vars.onnx__Concat_674 = reshape(Vars.onnx__Mul_585, shape);

% Unsqueeze:
[shape, NumDims.onnx__Concat_675] = prepareUnsqueezeArgs(Vars.onnx__Mul_588, Vars.UnsqueezeAxes1118, NumDims.onnx__Mul_588);
Vars.onnx__Concat_675 = reshape(Vars.onnx__Mul_588, shape);

% Concat:
[Vars.onnx__Reshape_676, NumDims.onnx__Reshape_676] = onnxConcat(0, {Vars.onnx__Concat_904, Vars.onnx__Concat_673, Vars.onnx__Concat_674, Vars.onnx__Concat_675}, [NumDims.onnx__Concat_904, NumDims.onnx__Concat_673, NumDims.onnx__Concat_674, NumDims.onnx__Concat_675]);

% Reshape:
[shape, NumDims.input_123] = prepareReshapeArgs(Vars.onnx__Reshape_666, Vars.onnx__Reshape_676, NumDims.onnx__Reshape_666, 0);
Vars.input_123 = reshape(Vars.onnx__Reshape_666, shape{:});

% MatMul:
[Vars.onnx__MatMul_684, NumDims.onnx__MatMul_684] = onnxMatMul(Vars.onnx__MatMul_908, Vars.dis_self_attn_snconv1x1_attn_weight_v, NumDims.onnx__MatMul_908, NumDims.dis_self_attn_snconv1x1_attn_weight_v);

% MatMul:
[Vars.onnx__Div_685, NumDims.onnx__Div_685] = onnxMatMul(Vars.dis_self_attn_snconv1x1_attn_weight_u, Vars.onnx__MatMul_684, NumDims.dis_self_attn_snconv1x1_attn_weight_u, NumDims.onnx__MatMul_684);

% Div:
Vars.onnx__Conv_686 = Vars.dis_self_attn_snconv1x1_attn_weight_orig ./ Vars.onnx__Div_685;
NumDims.onnx__Conv_686 = max(NumDims.dis_self_attn_snconv1x1_attn_weight_orig, NumDims.onnx__Div_685);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Mul_687] = prepareConvArgs(Vars.onnx__Conv_686, Vars.dis_self_attn_snconv1x1_attn_bias, Vars.ConvStride1119, Vars.ConvDilationFactor1120, Vars.ConvPadding1121, 1, NumDims.input_123, NumDims.onnx__Conv_686);
Vars.onnx__Mul_687 = dlconv(Vars.input_123, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Mul:
Vars.onnx__Add_688 = Vars.dis_self_attn_sigma .* Vars.onnx__Mul_687;
NumDims.onnx__Add_688 = max(NumDims.dis_self_attn_sigma, NumDims.onnx__Mul_687);

% Add:
Vars.input_127 = Vars.input_115 + Vars.onnx__Add_688;
NumDims.input_127 = max(NumDims.input_115, NumDims.onnx__Add_688);

% Relu:
Vars.onnx__Conv_690 = relu(Vars.input_127);
NumDims.onnx__Conv_690 = NumDims.input_127;

% MatMul:
[Vars.onnx__MatMul_697, NumDims.onnx__MatMul_697] = onnxMatMul(Vars.onnx__MatMul_912, Vars.dis_block2_snconv2d1_weight_v, NumDims.onnx__MatMul_912, NumDims.dis_block2_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_698, NumDims.onnx__Div_698] = onnxMatMul(Vars.dis_block2_snconv2d1_weight_u, Vars.onnx__MatMul_697, NumDims.dis_block2_snconv2d1_weight_u, NumDims.onnx__MatMul_697);

% Div:
Vars.onnx__Conv_699 = Vars.dis_block2_snconv2d1_weight_orig ./ Vars.onnx__Div_698;
NumDims.onnx__Conv_699 = max(NumDims.dis_block2_snconv2d1_weight_orig, NumDims.onnx__Div_698);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_700] = prepareConvArgs(Vars.onnx__Conv_699, Vars.dis_block2_snconv2d1_bias, Vars.ConvStride1122, Vars.ConvDilationFactor1123, Vars.ConvPadding1124, 1, NumDims.onnx__Conv_690, NumDims.onnx__Conv_699);
Vars.onnx__Pad_700 = dlconv(Vars.onnx__Conv_690, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_701, NumDims.onnx__AveragePool_701] = onnxPad(Vars.onnx__Pad_700, Vars.PadPadding1125, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_700]'), NumDims.onnx__Pad_700);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_702] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1126, Vars.AveragePoolStride1127, Vars.AveragePoolPadding1128, 0, NumDims.onnx__AveragePool_701);
Vars.onnx__Add_702 = avgpool(Vars.onnx__AveragePool_701, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% MatMul:
[Vars.onnx__MatMul_709, NumDims.onnx__MatMul_709] = onnxMatMul(Vars.onnx__MatMul_916, Vars.dis_block2_snconv2d0_weight_v, NumDims.onnx__MatMul_916, NumDims.dis_block2_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_710, NumDims.onnx__Div_710] = onnxMatMul(Vars.dis_block2_snconv2d0_weight_u, Vars.onnx__MatMul_709, NumDims.dis_block2_snconv2d0_weight_u, NumDims.onnx__MatMul_709);

% Div:
Vars.onnx__Conv_711 = Vars.dis_block2_snconv2d0_weight_orig ./ Vars.onnx__Div_710;
NumDims.onnx__Conv_711 = max(NumDims.dis_block2_snconv2d0_weight_orig, NumDims.onnx__Div_710);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_712] = prepareConvArgs(Vars.onnx__Conv_711, Vars.dis_block2_snconv2d0_bias, Vars.ConvStride1129, Vars.ConvDilationFactor1130, Vars.ConvPadding1131, 1, NumDims.onnx__Conv_690, NumDims.onnx__Conv_711);
Vars.onnx__Pad_712 = dlconv(Vars.onnx__Conv_690, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_713, NumDims.onnx__AveragePool_713] = onnxPad(Vars.onnx__Pad_712, Vars.PadPadding1132, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_712]'), NumDims.onnx__Pad_712);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_714] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1133, Vars.AveragePoolStride1134, Vars.AveragePoolPadding1135, 0, NumDims.onnx__AveragePool_713);
Vars.onnx__Add_714 = avgpool(Vars.onnx__AveragePool_713, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Add:
Vars.input_131 = Vars.onnx__Add_702 + Vars.onnx__Add_714;
NumDims.input_131 = max(NumDims.onnx__Add_702, NumDims.onnx__Add_714);

% Relu:
Vars.onnx__Conv_716 = relu(Vars.input_131);
NumDims.onnx__Conv_716 = NumDims.input_131;

% MatMul:
[Vars.onnx__MatMul_723, NumDims.onnx__MatMul_723] = onnxMatMul(Vars.onnx__MatMul_920, Vars.dis_block3_snconv2d1_weight_v, NumDims.onnx__MatMul_920, NumDims.dis_block3_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_724, NumDims.onnx__Div_724] = onnxMatMul(Vars.dis_block3_snconv2d1_weight_u, Vars.onnx__MatMul_723, NumDims.dis_block3_snconv2d1_weight_u, NumDims.onnx__MatMul_723);

% Div:
Vars.onnx__Conv_725 = Vars.dis_block3_snconv2d1_weight_orig ./ Vars.onnx__Div_724;
NumDims.onnx__Conv_725 = max(NumDims.dis_block3_snconv2d1_weight_orig, NumDims.onnx__Div_724);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_726] = prepareConvArgs(Vars.onnx__Conv_725, Vars.dis_block3_snconv2d1_bias, Vars.ConvStride1136, Vars.ConvDilationFactor1137, Vars.ConvPadding1138, 1, NumDims.onnx__Conv_716, NumDims.onnx__Conv_725);
Vars.onnx__Pad_726 = dlconv(Vars.onnx__Conv_716, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_727, NumDims.onnx__AveragePool_727] = onnxPad(Vars.onnx__Pad_726, Vars.PadPadding1139, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_726]'), NumDims.onnx__Pad_726);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_728] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1140, Vars.AveragePoolStride1141, Vars.AveragePoolPadding1142, 0, NumDims.onnx__AveragePool_727);
Vars.onnx__Add_728 = avgpool(Vars.onnx__AveragePool_727, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% MatMul:
[Vars.onnx__MatMul_735, NumDims.onnx__MatMul_735] = onnxMatMul(Vars.onnx__MatMul_924, Vars.dis_block3_snconv2d0_weight_v, NumDims.onnx__MatMul_924, NumDims.dis_block3_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_736, NumDims.onnx__Div_736] = onnxMatMul(Vars.dis_block3_snconv2d0_weight_u, Vars.onnx__MatMul_735, NumDims.dis_block3_snconv2d0_weight_u, NumDims.onnx__MatMul_735);

% Div:
Vars.onnx__Conv_737 = Vars.dis_block3_snconv2d0_weight_orig ./ Vars.onnx__Div_736;
NumDims.onnx__Conv_737 = max(NumDims.dis_block3_snconv2d0_weight_orig, NumDims.onnx__Div_736);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_738] = prepareConvArgs(Vars.onnx__Conv_737, Vars.dis_block3_snconv2d0_bias, Vars.ConvStride1143, Vars.ConvDilationFactor1144, Vars.ConvPadding1145, 1, NumDims.onnx__Conv_716, NumDims.onnx__Conv_737);
Vars.onnx__Pad_738 = dlconv(Vars.onnx__Conv_716, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_739, NumDims.onnx__AveragePool_739] = onnxPad(Vars.onnx__Pad_738, Vars.PadPadding1146, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_738]'), NumDims.onnx__Pad_738);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_740] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1147, Vars.AveragePoolStride1148, Vars.AveragePoolPadding1149, 0, NumDims.onnx__AveragePool_739);
Vars.onnx__Add_740 = avgpool(Vars.onnx__AveragePool_739, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Add:
Vars.input_135 = Vars.onnx__Add_728 + Vars.onnx__Add_740;
NumDims.input_135 = max(NumDims.onnx__Add_728, NumDims.onnx__Add_740);

% Relu:
Vars.onnx__Conv_742 = relu(Vars.input_135);
NumDims.onnx__Conv_742 = NumDims.input_135;

% MatMul:
[Vars.onnx__MatMul_749, NumDims.onnx__MatMul_749] = onnxMatMul(Vars.onnx__MatMul_928, Vars.dis_block4_snconv2d1_weight_v, NumDims.onnx__MatMul_928, NumDims.dis_block4_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_750, NumDims.onnx__Div_750] = onnxMatMul(Vars.dis_block4_snconv2d1_weight_u, Vars.onnx__MatMul_749, NumDims.dis_block4_snconv2d1_weight_u, NumDims.onnx__MatMul_749);

% Div:
Vars.onnx__Conv_751 = Vars.dis_block4_snconv2d1_weight_orig ./ Vars.onnx__Div_750;
NumDims.onnx__Conv_751 = max(NumDims.dis_block4_snconv2d1_weight_orig, NumDims.onnx__Div_750);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_752] = prepareConvArgs(Vars.onnx__Conv_751, Vars.dis_block4_snconv2d1_bias, Vars.ConvStride1150, Vars.ConvDilationFactor1151, Vars.ConvPadding1152, 1, NumDims.onnx__Conv_742, NumDims.onnx__Conv_751);
Vars.onnx__Pad_752 = dlconv(Vars.onnx__Conv_742, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_753, NumDims.onnx__AveragePool_753] = onnxPad(Vars.onnx__Pad_752, Vars.PadPadding1153, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_752]'), NumDims.onnx__Pad_752);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_754] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1154, Vars.AveragePoolStride1155, Vars.AveragePoolPadding1156, 0, NumDims.onnx__AveragePool_753);
Vars.onnx__Add_754 = avgpool(Vars.onnx__AveragePool_753, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% MatMul:
[Vars.onnx__MatMul_761, NumDims.onnx__MatMul_761] = onnxMatMul(Vars.onnx__MatMul_932, Vars.dis_block4_snconv2d0_weight_v, NumDims.onnx__MatMul_932, NumDims.dis_block4_snconv2d0_weight_v);

% MatMul:
[Vars.onnx__Div_762, NumDims.onnx__Div_762] = onnxMatMul(Vars.dis_block4_snconv2d0_weight_u, Vars.onnx__MatMul_761, NumDims.dis_block4_snconv2d0_weight_u, NumDims.onnx__MatMul_761);

% Div:
Vars.onnx__Conv_763 = Vars.dis_block4_snconv2d0_weight_orig ./ Vars.onnx__Div_762;
NumDims.onnx__Conv_763 = max(NumDims.dis_block4_snconv2d0_weight_orig, NumDims.onnx__Div_762);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Pad_764] = prepareConvArgs(Vars.onnx__Conv_763, Vars.dis_block4_snconv2d0_bias, Vars.ConvStride1157, Vars.ConvDilationFactor1158, Vars.ConvPadding1159, 1, NumDims.onnx__Conv_742, NumDims.onnx__Conv_763);
Vars.onnx__Pad_764 = dlconv(Vars.onnx__Conv_742, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Pad:
[Vars.onnx__AveragePool_765, NumDims.onnx__AveragePool_765] = onnxPad(Vars.onnx__Pad_764, Vars.PadPadding1160, 0.000000, 'constant', dlarray([0:NumDims.onnx__Pad_764]'), NumDims.onnx__Pad_764);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.onnx__Add_766] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1161, Vars.AveragePoolStride1162, Vars.AveragePoolPadding1163, 0, NumDims.onnx__AveragePool_765);
Vars.onnx__Add_766 = avgpool(Vars.onnx__AveragePool_765, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Add:
Vars.input_139 = Vars.onnx__Add_754 + Vars.onnx__Add_766;
NumDims.input_139 = max(NumDims.onnx__Add_754, NumDims.onnx__Add_766);

% Relu:
Vars.onnx__Conv_768 = relu(Vars.input_139);
NumDims.onnx__Conv_768 = NumDims.input_139;

% MatMul:
[Vars.onnx__MatMul_775, NumDims.onnx__MatMul_775] = onnxMatMul(Vars.onnx__MatMul_936, Vars.dis_block5_snconv2d1_weight_v, NumDims.onnx__MatMul_936, NumDims.dis_block5_snconv2d1_weight_v);

% MatMul:
[Vars.onnx__Div_776, NumDims.onnx__Div_776] = onnxMatMul(Vars.dis_block5_snconv2d1_weight_u, Vars.onnx__MatMul_775, NumDims.dis_block5_snconv2d1_weight_u, NumDims.onnx__MatMul_775);

% Div:
Vars.onnx__Conv_777 = Vars.dis_block5_snconv2d1_weight_orig ./ Vars.onnx__Div_776;
NumDims.onnx__Conv_777 = max(NumDims.dis_block5_snconv2d1_weight_orig, NumDims.onnx__Div_776);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.onnx__Add_778] = prepareConvArgs(Vars.onnx__Conv_777, Vars.dis_block5_snconv2d1_bias, Vars.ConvStride1164, Vars.ConvDilationFactor1165, Vars.ConvPadding1166, 1, NumDims.onnx__Conv_768, NumDims.onnx__Conv_777);
Vars.onnx__Add_778 = dlconv(Vars.onnx__Conv_768, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.input_143 = Vars.onnx__Add_778 + Vars.onnx__Conv_768;
NumDims.input_143 = max(NumDims.onnx__Add_778, NumDims.onnx__Conv_768);

% Relu:
Vars.onnx__Reshape_780 = relu(Vars.input_143);
NumDims.onnx__Reshape_780 = NumDims.input_143;

% Reshape:
[shape, NumDims.onnx__Gemm_782] = prepareReshapeArgs(Vars.onnx__Reshape_780, Vars.onnx__Reshape_781, NumDims.onnx__Reshape_780, 0);
Vars.onnx__Gemm_782 = reshape(Vars.onnx__Reshape_780, shape{:});

% MatMul:
[Vars.onnx__MatMul_789, NumDims.onnx__MatMul_789] = onnxMatMul(Vars.onnx__MatMul_940, Vars.dis_snlinear2_weight_v, NumDims.onnx__MatMul_940, NumDims.dis_snlinear2_weight_v);

% MatMul:
[Vars.onnx__Div_790, NumDims.onnx__Div_790] = onnxMatMul(Vars.dis_snlinear2_weight_u, Vars.onnx__MatMul_789, NumDims.dis_snlinear2_weight_u, NumDims.onnx__MatMul_789);

% Div:
Vars.onnx__Gemm_791 = Vars.dis_snlinear2_weight_orig ./ Vars.onnx__Div_790;
NumDims.onnx__Gemm_791 = max(NumDims.dis_snlinear2_weight_orig, NumDims.onnx__Div_790);

% Gemm:
[A, B, C, alpha, beta, NumDims.onnx__Sigmoid_792] = prepareGemmArgs(Vars.onnx__Gemm_782, Vars.onnx__Gemm_791, Vars.dis_snlinear2_bias, Vars.Gemmalpha1167, Vars.Gemmbeta1168, 0, 1, NumDims.dis_snlinear2_bias);
Vars.onnx__Sigmoid_792 = alpha*B*A + beta*C;

% Sigmoid:
Vars.onnx__Squeeze_793 = sigmoid(Vars.onnx__Sigmoid_792);
NumDims.onnx__Squeeze_793 = NumDims.onnx__Sigmoid_792;

% Squeeze:
[Vars.Y, NumDims.Y] = onnxSqueeze(Vars.onnx__Squeeze_793, Vars.SqueezeAxes1169, NumDims.onnx__Squeeze_793);

% Set graph output arguments from Vars and NumDims:
Y = Vars.Y;
YNumDims1171 = NumDims.Y;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(X, numDataOutputs, params, varargin)
% Function to validate inputs to MyNet:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'X', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, X, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [X, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(X, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(X, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {X}));
% Make the input variables into unlabelled dlarrays:
X = makeUnlabeledDlarray(X);
% Permute inputs if requested:
X = permuteInputVar(X, inputDataPerms{1}, 2);
% Check input size(s):
checkInputSize(size(X), {1 5}, "X");
end

function [Y] = postprocessOutput(Y, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    if isdlarray(Y)
        Y = extractdata(Y);
    end
end
% Permute outputs if requested:
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY] = onnxConcat(ONNXAxis, XCell, numDimsXArray)
% Concatentation that treats all empties the same. Necessary because
% dlarray.cat does not allow, for example, cat(1, 1x1, 1x0) because the
% second dimension sizes do not match.

% Copyright 2021 The MathWorks, Inc.

numDimsY = numDimsXArray(1);
XCell(cellfun(@isempty, XCell)) = [];
if isempty(XCell)
    Y = dlarray([]);
else
    if ONNXAxis<0
        ONNXAxis = ONNXAxis + numDimsY;
    end
    DLTAxis = numDimsY - ONNXAxis;
    Y = cat(DLTAxis, XCell{:});
end
end

function [Y, numDimsY] = onnxGather(X, ONNXIdx, ONNXAxis, numDimsX, numDimsIdx)
% Function implementing the ONNX Gather operator

% In ONNX, 'Gather' first indexes into dimension ONNXAxis of data, using
% the contents of ONNXIdx as the indices. Then, it reshapes the ONNXAxis
% into the shape of ONNXIdx.
%   Example 1:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6 7], and axis=1.
% The result has shape [2 6 7 4 5].
%   Example 2:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6], and axis=1.
% The result has shape [2 6 4 5].
%   Example 3:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [] (a scalar), and axis=1.
% The result has shape [2 4 5].
%
% Since we're using reverse indexing relative to ONNX, in this function
% data and ONNXIdx both have reversed dimension ordering.

% Copyright 2020-2021 The MathWorks, Inc.

numDimsY = numDimsIdx + (numDimsX - 1);
if isempty(X)
    Y = X;
    return;
end
% (1) First, do the subsref part of Gather
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;                                 % Axis can be negative. Convert it to its positive equivalent.
end
dltAxis = numDimsX - ONNXAxis;                                      % Convert axis to DLT. ONNXAxis is origin 0 and we index from the end
ONNXIdx(ONNXIdx<0) = ONNXIdx(ONNXIdx<0) + size(X, dltAxis);         % ONNXIdx can have negative components. Make them positive.
dltIdx  = extractdata(ONNXIdx) + 1;                                 % ONNXIdx is origin-0 in ONNX, so add 1 to get dltIdx
% Use subsref to index into data
Indices.subs = repmat({':'}, 1, numDimsX);
Indices.subs{dltAxis} = dltIdx(:);                                  % Index as a column to ensure the output is 1-D in the indexed dimension (for now).
Indices.type = '()';
Y = subsref(X, Indices);
% (2) Now do the reshaping part of Gather
shape = size(Y, 1:numDimsX);
if numDimsIdx == 0
    % Delete the indexed dimension
    shape(dltAxis) = [];
elseif numDimsIdx > 1
    % Reshape the indexed dimension into the shape of ONNXIdx
    shape = [shape(1:dltAxis-1) size(ONNXIdx, 1:numDimsIdx) shape(dltAxis+1:end)];
end
% Extend the shape to 2D so it's valid MATLAB
if numel(shape) < 2
    shape = [shape ones(1,2-numel(shape))];
end
Y = reshape(Y, shape);
end

function [D, numDimsD] = onnxMatMul(A, B, numDimsA, numDimsB)
% Implements the ONNX MatMul operator.

% Copyright 2020-2023 The MathWorks, Inc.

% If B is 1-D, temporarily extend it to a row vector
if numDimsB==1
    B = B(:)';
end
maxNumDims = max(numDimsA, numDimsB);
numDimsD = maxNumDims;
if maxNumDims > 2
    % Removes dlarray formats if only one of the input dlarrays is formatted.
    if sum([isempty(dims(A)), isempty(dims(B))]) == 1
        D = pagemtimes(stripdims(B), stripdims(A));
    else
        %computes matrix product of corresponding pages of input arrays A and
        %B.
        D = pagemtimes(B, A);
    end
else
    D = B * A;
    if numDimsA==1 || numDimsB==1
        D = D(:);
        numDimsD = 1;
    end
end
end

function [Y, numDimsY] = onnxPad(X, pads, value, mode, ONNXAxis, numDimsX)
% Implements the ONNX Pad operator

% ONNX 'pads' is a vector: [x1_begin, x2_begin...x1_end, x2_end,...], with
% x1,x2, listed in FORWARD ONNX dimension ordering, because it is data
% within a dimension and so is not flipped. xi_begin is the number of
% pixels added at the beginning of axis `i` and xi_end, the number of
% pixels added at the end of axis `i`.  pads can be negative, in which case
% that number of pixels is removed.

% Copyright 2020-2024 The MathWorks, Inc.

pads = pads(:)';
numDimsY = numDimsX;
if ONNXAxis < 0
    ONNXAxis = ONNXAxis + numDimsX;
end
% Fill in pads to length 2*numDimsX if size(ONNXAxis,1) < numDimsX
if size(ONNXAxis,1) < numDimsX
    helpPads = dlarray(zeros(1,2*numDimsX));
    helpPads([ONNXAxis+1,ONNXAxis+numDimsX+1]) = pads;
    pads = helpPads;
end

if numDimsX==1
    % X is Nx1. Temporarily make it reverse-ONNX 2D (1xN), then transpose
    % the result back to 1D at the end.
    X = X';
    numDimsX = 2;
    pads = [pads(1) 0 pads(2) 0];  % Don't pad the dummy dimension
    numDimsY = 1;
end
sizeX  = size(X, 1:numDimsX);
fwdPadMat = reshape(extractdata(pads), [], 2)';  % row1 = begins, row2 = ends
% Columns of padmat are in reverse ONNX ordering. Still the case that row1
% = begins, row2 = ends:
padmat = fliplr(fwdPadMat);
sizeY  = sum([sizeX; padmat]);
% Create output tensor of the right size
Y = value*ones(sizeY, 'like', X);
% Construct subsref indices for inserting (and cropping) the original
for i=1:numel(sizeX)
    Ysubs{i} = max(1,1+padmat(1,i)) : min(sizeY(i), sizeY(i)-padmat(2,i));
    Xsubs{i} = max(1,1-padmat(1,i)) : min(sizeX(i), sizeX(i)+padmat(2,i));
end
Sy      = struct('type', '()');
Sy.subs = Ysubs;
Sx      = struct('type', '()');
Sx.subs = Xsubs;
% Insert/crop the original into the result
Y = subsasgn(Y, Sy, subsref(X, Sx));
% Handle 'reflect' and 'edge' modes, but don't do it if X was 1D, 0x1.
if ismember(mode, ["edge", "reflect"]) && ~(numDimsY==1 && sizeX(2)==0)
    for dim = 1:numDimsX
        if any(padmat(:,dim)>0)
            % Setup a call to subsasgn
            prepad  = padmat(1,dim);
            postpad = padmat(2,dim);
            if prepad > 0
                [Sy, Sx] = prepadIndices(sizeX, prepad, dim, mode);
                Y = subsasgn(Y, Sy, subsref(Y, Sx));
            end
            if postpad > 0
                [Sy, Sx] = postpadIndices(sizeX, sizeY, prepad, postpad, dim, mode);
                Y = subsasgn(Y, Sy, subsref(Y, Sx));
            end
        end
    end
end
% Transpose the result back to 1D if the input was 1D
if numDimsY==1
    Y = Y';
end

% Subfunctions in onnxPad:
    function [Sy, Sx] = prepadIndices(sizeX, prepad, dim, mode)
        Sy   	= struct('type', '()');
        Sy.subs	= repmat({':'}, [1 numel(sizeX)]);
        Sx   	= Sy;
        % Write into the first 'prepad' elements of Y.dim.
        Sy.subs{dim} = 1:prepad;
        switch mode
            case 'reflect'
                % Create indices 2:prepad+1 of X.dim, in the reverse order, with
                % wraparound. Then add prepad to convert them to Y indices.
                Sx.subs{dim} = wrapIndices(prepad+1 : -1 : 2, sizeX(dim)) + prepad;
            case 'edge'
                % Create replicated indices 1 of X.dim. Then add prepad to
                % convert them to Y indices.
                Sx.subs{dim} = repmat(1, [1 prepad]) + prepad;
            otherwise
                assert(false);
        end
    end

    function [Sy, Sx] = postpadIndices(sizeX, sizeY, prepad, postpad, dim, mode)
        Sy   	= struct('type', '()');
        Sy.subs	= repmat({':'}, [1 numel(sizeX)]);
        Sx   	= Sy;
        % Write into the last 'postpad' elements of Y.dim.
        Sy.subs{dim} = sizeY(dim)-postpad+1 : sizeY(dim);
        switch mode
            case 'reflect'
                % Create indices in the reverse order, with wraparound. Then add
                % prepad to convert them to Y indices.
                Sx.subs{dim} = wrapIndices(sizeX(dim)-1 : -1 : sizeX(dim)-postpad, sizeX(dim)) + prepad;
            case 'edge'
                % Create replicated end indices . Then add prepad to convert them
                % to Y indices.
                Sx.subs{dim} = repmat(sizeX(dim), [1 postpad]) + prepad;
            otherwise
                assert(false);
        end
    end

    function j = wrapIndices(i, maxIdx)
        % i can be positive, negative or zero. Legal output indices are in the
        % range 1:maxIdx.
        j = mod(i-1, maxIdx) + 1;
    end
end


function [Y, numDimsY] = onnxShape(X, numDimsX, startAxis, endAxis)
% Implements the ONNX Shape operator
% Return the reverse ONNX shape as a 1D column vector

% Copyright 2020-2024 The MathWorks, Inc.

switch numDimsX
    case 0
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(1);
        end
    case 1
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(size(X,1));
        end
    otherwise
        if(endAxis<0)
            %  If the endAxis is smaller than 0 after converting it positive,
            % the endAxis is 0
            endAxis = max(0, numDimsX + endAxis);
        end
        if(startAxis<0)
            %  If the startAxis is smaller than 0 after converting it positive,
            % the startAxis is 0
            startAxis = max(0, numDimsX + startAxis);
        end
        % transform startAxis and endAxis from 0 index to 1 index
        startAxis = startAxis + 1;
        endAxis = endAxis + 1;
        % if startAxis is larger than numDimsX or endAxis is larger than
        % numDimsX + 1, cramp it to the upper bound. The endAxis is exclusive,
        % transform it to MATLAB inclusive way
        endAxis = min(endAxis, numDimsX + 1) - 1;
        startAxis = min(startAxis, numDimsX);
        if endAxis < startAxis || endAxis == 0
            Y = dlarray(0);
        else
            Y = dlarray(fliplr(size(X, (numDimsX-endAxis+1):(numDimsX-startAxis+1)))');
        end
end
numDimsY = 1;
end

function [Y, numDimsY] = onnxSqueeze(X, ONNXAxes, numDimsX)
% Implements the ONNX Squeeze operator

% Copyright 2020 The MathWorks, Inc.

if numDimsX == 0
    Y = X;
    numDimsY = numDimsX;
else
    % Find the new ONNX shape
    curOShape = size(X, numDimsX:-1:1);
    if isempty(ONNXAxes)
        newOShape = curOShape(curOShape ~= 1);
    else
        ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
        newOShape = curOShape;
        newOShape(ONNXAxes+1) = [];
    end
    % Get numDimsY from ONNX shape
    numDimsY  = numel(newOShape);
    newMShape = [fliplr(newOShape) ones(1, 2-numDimsY)];    % Append 1's to shape if numDims<2
    Y         = reshape(X, newMShape);
end
end

function [poolSize, stride, padding, paddingValue, dataFormat, numDimsY] = prepareAveragePoolArgs(poolSize, stride, padding, count_include_pad, numDimsX)

%   Copyright 2020-2020 The MathWorks, Inc.

% Prepares arguments for implementing the ONNX AveragePool operator
poolSize    = fliplr(extractdata(poolSize(:)'));
stride      = fliplr(extractdata(stride(:)'));
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the fliplr and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
if logical(count_include_pad)
    paddingValue = 0;
else
    paddingValue = 'mean';
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [offset, scale, datasetMean, datasetVariance, dataFormat, numDimsY, numDimsDatasetMean, numDimsDatasetVariance] = prepareBatchNormalizationArgs(...
    offset, scale, datasetMean, datasetVariance, numDimsX, numDimsDatasetMean, numDimsDatasetVariance)
% Prepares arguments for implementing the ONNX BatchNormalization operator

%   Copyright 2020-2021 The MathWorks, Inc.

offset = dlarray(offset,'C');
scale = dlarray(scale,'C');
datasetMean = extractdata(datasetMean);
datasetVariance = extractdata(datasetVariance);
datasetVariance(datasetVariance <= 0) = realmin('single');  % Set nonpositive variance components to a value below eps('single')
dataFormat = [repmat('S', 1, numDimsX-2), 'CB'];
numDimsY = numDimsX;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

%   Copyright 2020 The MathWorks, Inc.

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Derive missing default attributes from weight tensor
numSpatialDims = numDimsW-2;
if isempty(padding)
    padding = zeros(1, 2*numSpatialDims);
end
if isempty(stride)
    stride = ones(1,numSpatialDims);
end
if isempty(dilationFactor)
    dilationFactor = ones(1,numSpatialDims);
end
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes double row vectors, and flip their dimension ordering
% to reverse-onnx:
stride = fliplr(double(stride(:)'));
dilationFactor = fliplr(double(dilationFactor(:)'));
if isnumeric(padding)       % padding can be "same"
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [A, B, C, alpha, beta, numDimsY] = prepareGemmArgs(A, B, C, alpha, beta, transA, transB, numDimsC)

%   Copyright 2020 The MathWorks, Inc.

% Prepares arguments for implementing the ONNX Gemm operator
if transA
    A = A';
end
if transB
    B = B';
end
if numDimsC < 2
    C = C(:);   % C can be broadcast to [N M]. Make C a col vector ([N 1])
end
numDimsY = 2;
% Y=B*A because we want (AB)'=B'A', and B and A are already transposed.
end

function [poolsize, stride, padding, dataFormat, numDimsY, numDimsIndices] = prepareMaxPool8Args(poolsize, stride, padding, numDimsX)
% Prepares arguments for implementing the ONNX MaxPool-8 operator

%   Copyright 2020 The MathWorks, Inc.

poolsize    = fliplr(extractdata(poolsize(:)'));
stride      = fliplr(extractdata(stride(:)'));
% padding
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY    = numDimsX;
numDimsIndices = numDimsX;                      % New in opset 8
end

function [DLTShape, numDimsY] = prepareReshapeArgs(X, ONNXShape, numDimsX, allowzero)
% Prepares arguments for implementing the ONNX Reshape operator

%   Copyright 2020-2024 The MathWorks, Inc.

ONNXShape = flip(extractdata(ONNXShape));            % First flip the shape to make it correspond to the dimensions of X.
% In ONNX, 0 means "unchanged" if allowzero is false, and -1 means "infer". In DLT, there is no
% "unchanged", and [] means "infer".
DLTShape = num2cell(ONNXShape);                      % Make a cell array so we can include [].
% Replace zeros with the actual size if allowzero is false
if any(ONNXShape==0) && allowzero==0
    i0 = find(ONNXShape==0);
    DLTShape(i0) = num2cell(size(X, numDimsX - numel(ONNXShape) + i0));  % right-align the shape vector and dims
end
if any(ONNXShape == -1)
    % Replace -1 with []
    i = ONNXShape == -1;
    DLTShape{i} = [];
end
if numel(DLTShape)==1
    DLTShape = [DLTShape 1];
end
numDimsY = numel(ONNXShape);
end

function [DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, numDimsY] = prepareResize11Args(...
    ONNXRoi, ONNXScales, ONNXSizes, coordinate_transformation_mode, mode, nearest_mode, numDimsX)
% Prepares arguments for implementing the ONNX Resize-11 operator

%   Copyright 2020-2024 The MathWorks, Inc.

% ONNXScales and ONNXSizes are in ONNX dimension ordering. ONNXRoi is
% ignored because it only takes effect when coordinate_transformation_mode
% is "tf_crop_and_resize", which is not supported.
DLTScales = flip(extractdata(ONNXScales(:)'));
DLTSizes = flip(extractdata(ONNXSizes(:)'));
switch coordinate_transformation_mode
    case "half_pixel"
        GeometricTransformMode = "half-pixel";
    case "asymmetric"
        GeometricTransformMode = "asymmetric";
    otherwise
        assert(false);
end
switch mode
    case "nearest"
        Method = "nearest";
    case "linear"
        Method = "linear";
    otherwise
        assert(false);
end
switch nearest_mode
    case "floor"
        NearestRoundingMode = "floor";
    otherwise
        NearestRoundingMode = "round";
end
dataFormat = repmat('S', [1 numDimsX]);
numDimsY = numDimsX;
end

function [dim1, dim2, origSize, numDimsX] = prepareSoftmaxArgs(X, ONNXAxis, numDimsX)
% Prepares arguments for implementing the ONNX Softmax operator

%   Copyright 2020 The MathWorks, Inc.

if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;
end
dim2     = prod(size(X, numDimsX+1-ONNXAxis:numDimsX));   % numel on the right
dim1     = numel(X)/dim2;                                 % numel on the left
origSize = size(X);
end

function [perm, numDimsA] = prepareTransposeArgs(ONNXPerm, numDimsA)
% Prepares arguments for implementing the ONNX Transpose operator

%   Copyright 2020 The MathWorks, Inc.

if numDimsA <= 1        % Tensors of numDims 0 or 1 are unchanged by ONNX Transpose.
    perm = [];
else
    if isempty(ONNXPerm)        % Empty ONNXPerm means reverse the dimensions.
        perm = numDimsA:-1:1;
    else
        perm = numDimsA-flip(ONNXPerm);
    end
end
end

function [newShape, numDimsY] = prepareUnsqueezeArgs(X, ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Unsqueeze operator

%   Copyright 2020-2021 The MathWorks, Inc.

numDimsY = numDimsX + numel(ONNXAxes);
ONNXAxes = extractdata(ONNXAxes);
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsY;
ONNXAxes = sort(ONNXAxes);                                              % increasing order
if numDimsY == 1
    newShape = size(X);
else
    DLTAxes  = flip(numDimsY - ONNXAxes);                                  % increasing order
    newShape = ones(1, numDimsY);
    posToSet = setdiff(1:numDimsY, DLTAxes, 'stable');
    newShape(posToSet) = size(X, 1:numel(posToSet));
end
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.

%   Copyright 2020 The MathWorks, Inc.

if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

%   Copyright 2020-2021 The MathWorks, Inc.

if numel(expectedShape)==0
    % The input is a scalar
    if ~isequal(inputShape, [1 1])
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, "[1,1]", inputSizeStr));
    end
elseif numel(expectedShape)==1
    % The input is a vector
    if ~shapeIsColumnVector(inputShape) || ~iSizesMatch({inputShape(1)}, expectedShape)
        expectedShape{2} = 1;
        expectedSizeStr = makeSizeString(expectedShape);
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
else
    % The input has 2 dimensions or more

    % The input dimensions have been reversed; flip them back to compare to the
    % expected ONNX shape.
    inputShape = fliplr(inputShape);

    % If the expected shape has fewer dims than the input shape, error.
    if numel(expectedShape) < numel(inputShape)
        expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
        error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
    end

    % Prepad the input shape with trailing ones up to the number of elements in
    % expectedShape
    inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);

    % Find the number of variable size dimensions in the expected shape
    numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));

    % Find the number of input dimensions that are not in the expected shape
    % and cannot be represented by a variable dimension
    nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
    numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;

    expectedSizeStr = makeSizeString(expectedShape);
    inputSizeStr = makeSizeString(inputShape);
    if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
        % The actual and expected input dimensions match, but in
        % a different order. The input needs to be permuted.
        error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
    elseif numNonMatchingInputDims > 0
        % The actual and expected input sizes do not match.
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
end
end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function sizeStr = makeSizeString(shape)
sizeStr = strjoin(["[", strjoin(string(shape), ","), "]"], "");
end

function isVec = shapeIsColumnVector(shape)
if numel(shape) == 2 && shape(2) == 1
    isVec = true;
else
    isVec = false;
end
end
function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray

%   Copyright 2020-2021 The MathWorks, Inc.

if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if isinteger(X)
        % Make ints double so they can combine with anything without
        % reducing precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)

%   Copyright 2020 The MathWorks, Inc.

% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)

%   Copyright 2020-2021 The MathWorks, Inc.
% Returns reverse-ONNX ordering
if onnxNDims == 0
    return;
elseif onnxNDims == 1 && isvector(X)
    X = X(:);
    return;
elseif isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    if numel(userDataPerm) ~= onnxNDims
        error(message('nnet_cnn_onnx:onnx:InputPermutationSize', numel(userDataPerm), onnxNDims));
    end
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
elseif isequal(userDataPerm, 'as-is')
    % Do not permute the input
    perm = 1:ndims(X);
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end

function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)

%   Copyright 2020-2021 The MathWorks, Inc.
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        elseif isequal(userDataPerm, 'as-is')
            % Do not permute the input
            perm = 1:ndims(Y);
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in
% t.
%   Copyright 2020 The MathWorks, Inc.

for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
